{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Roboflow_Deploy_Custom_Mobilenet_to_OAK-D.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhargav23/OpenCV/blob/main/Deploy_Custom_Mobilenet_to_OAK_D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJz_ToJtkufM"
      },
      "source": [
        "# Roboflow and DepthAI Tutorial: Train and Deploy a custom object detection model with depth! \n",
        "\n",
        "#### ![Roboflow Workmark](https://i.imgur.com/WHFqYSJ.png)\n",
        "\n",
        "\n",
        "<img src=\"https://docs.luxonis.com/images/depthai_logo.png\" width=\"500\">\n",
        "\n",
        "We highly recommend working through this notebook with the corresponding blog post in hand. \n",
        "\n",
        "A Major shoutout to Luxonis, the creators of the OAK-D Device for originally putting this tutorial together. Here, we streamline the training process with data management, transformation, and exportation from Roboflow. \n",
        "\n",
        "At the end of this tutorial, you will have a custom trained object detection model to deploy to your OAK-D device!\n",
        "\n",
        "\n",
        "Insert GIF\n",
        "\n",
        "The steps we take to train and deploy our custom model are as follows \n",
        "\n",
        "\n",
        "* Install MobileNet Training Environment Dependencies\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TEgconcGvVK"
      },
      "source": [
        "# Install Training Environment Dependencies and Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH1x08R-yM-L"
      },
      "source": [
        "# %%capture\n",
        "#After this cell executes runtime will restart to finish the install, ignore and close the crash message, continue running cells starting with the one below\n",
        "#!pip install numpy==1.17.5;\n",
        "#import os\n",
        "#os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z8kgcs0E7iV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd973fb-6367-4192-af29-b893266acbc6"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install tf_slim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Collecting tf_slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 13.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf_slim) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n",
            "Installing collected packages: tf-slim\n",
            "Successfully installed tf-slim-1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnNXNQCjdniL"
      },
      "source": [
        "\n",
        "# Number of training steps - 1000 will train very quickly, but more steps will increase accuracy.\n",
        "\n",
        "num_steps = 3000  # A step means using a single batch of data. larger batch, less steps required\n",
        "#60000 steps is required to train our example sign language dataset, less or more may be required based on your custom datasets needs\n",
        "\n",
        "#Number of evaluation steps.\n",
        "num_eval_steps = 50\n",
        "#Batch size 24 is a setting that generally works well. can be changed higher or lower \n",
        "MODELS_CONFIG = {\n",
        "        'ssd_mobilenet_v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n",
        "        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n",
        "        'batch_size': 24\n",
        "    }\n",
        "}\n",
        "selected_model = 'ssd_mobilenet_v2'\n",
        "\n",
        "# Name of the object detection model to use.\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
        "\n",
        "# Name of the pipline file in tensorflow object detection API.\n",
        "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
        "\n",
        "# Training batch size fits in Colab's GPU memory for selected model.\n",
        "batch_size = MODELS_CONFIG[selected_model]['batch_size']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI8__uNS8-ns"
      },
      "source": [
        "# Install Tensorflow Object Detection API\n",
        "\n",
        "Clone TF models which contains the Object Detection API; also install the required dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecpHEnka8Kix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfae2856-0f7c-42a6-a9d2-8d66f890d4e8"
      },
      "source": [
        "# %%capture\n",
        "%cd /content\n",
        "!git clone --quiet https://github.com/tensorflow/models.git\n",
        "%cd /content/models/\n",
        "!git checkout 58d19c67e1d30d905dd5c6e5092348658fed80af\n",
        "!apt-get update && apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
        "!pip install -q pycocotools\n",
        "%cd /content/models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n",
        "!python object_detection/builders/model_builder_test.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "/content/models\n",
            "Note: checking out '58d19c67e1d30d905dd5c6e5092348658fed80af'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 58d19c67 Internal change\n",
            "Get:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:5 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:6 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:12 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,785 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:14 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:15 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [913 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,693 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [537 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,194 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [33.6 kB]\n",
            "Get:21 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.1 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [505 kB]\n",
            "Ign:25 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:26 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,257 kB]\n",
            "Get:25 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [680 kB]\n",
            "Get:27 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,420 kB]\n",
            "Fetched 13.4 MB in 3s (3,843 kB/s)\n",
            "Reading package lists... Done\n",
            "Selecting previously unselected package python-bs4.\n",
            "(Reading database ... 160837 files and directories currently installed.)\n",
            "Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n",
            "Unpacking python-bs4 (4.6.0-1) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-chardet.\n",
            "Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n",
            "Unpacking python-chardet (3.0.4-1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-webencodings.\n",
            "Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n",
            "Unpacking python-webencodings (0.5-2) ...\n",
            "Selecting previously unselected package python-html5lib.\n",
            "Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n",
            "Unpacking python-html5lib (0.999999999-1) ...\n",
            "Selecting previously unselected package python-lxml:amd64.\n",
            "Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.4_amd64.deb ...\n",
            "Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.4) ...\n",
            "Selecting previously unselected package python-olefile.\n",
            "Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n",
            "Unpacking python-olefile (0.45.1-1) ...\n",
            "Selecting previously unselected package python-pil:amd64.\n",
            "Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.6_amd64.deb ...\n",
            "Unpacking python-pil:amd64 (5.1.0-1ubuntu0.6) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python-bs4 (4.6.0-1) ...\n",
            "Setting up python-lxml:amd64 (4.2.1-1ubuntu0.4) ...\n",
            "Setting up python-olefile (0.45.1-1) ...\n",
            "Setting up python-pil:amd64 (5.1.0-1ubuntu0.6) ...\n",
            "Setting up python-webencodings (0.5-2) ...\n",
            "Setting up python-chardet (3.0.4-1) ...\n",
            "Setting up python-html5lib (0.999999999-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "/content/models/research\n",
            "object_detection/protos/input_reader.proto: warning: Import object_detection/protos/image_resizer.proto but not used.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT0asaLNN1Vz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "108911d0-7ba4-4bca-e1a5-4357f41fd4c9"
      },
      "source": [
        "import os\n",
        "\n",
        "repo_url = 'https://github.com/roboflow-ai/tensorflow-object-detection-faster-rcnn'\n",
        "\n",
        "%cd /content\n",
        "\n",
        "repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n",
        "\n",
        "!git clone {repo_url}\n",
        "%cd {repo_dir_path}\n",
        "!git pull\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'tensorflow-object-detection-faster-rcnn'...\n",
            "remote: Enumerating objects: 885, done.\u001b[K\n",
            "remote: Total 885 (delta 0), reused 0 (delta 0), pack-reused 885\u001b[K\n",
            "Receiving objects: 100% (885/885), 24.83 MiB | 9.33 MiB/s, done.\n",
            "Resolving deltas: 100% (428/428), done.\n",
            "/content/tensorflow-object-detection-faster-rcnn\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-k7uGThXlny"
      },
      "source": [
        "## Prepare `tfrecord` files\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcJhUwgCIKpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21d04688-7ead-4f13-9a17-827373053552"
      },
      "source": [
        "%cd /content/tensorflow-object-detection-faster-rcnn/data\n",
        "\n",
        "!curl -L \"https://app.roboflow.com/ds/h9Xe7Z2CYa?key=4aAgmw1RLw\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tensorflow-object-detection-faster-rcnn/data\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   891  100   891    0     0   1403      0 --:--:-- --:--:-- --:--:--  1403\n",
            "100  100M  100  100M    0     0  28.9M      0  0:00:03  0:00:03 --:--:-- 59.8M\n",
            "Archive:  roboflow.zip\n",
            " extracting: README.roboflow.txt     \n",
            "   creating: test/\n",
            " extracting: test/bus.tfrecord       \n",
            " extracting: test/bus_label_map.pbtxt  \n",
            "   creating: train/\n",
            " extracting: train/bus.tfrecord      \n",
            " extracting: train/bus_label_map.pbtxt  \n",
            "   creating: valid/\n",
            " extracting: valid/bus.tfrecord      \n",
            " extracting: valid/bus_label_map.pbtxt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4K8uaumM9zo"
      },
      "source": [
        "#RENAME Based on your annotation type - In this example our annotation type is \"letters\"\n",
        "#RENAME to the file path based on your download above ^^\n",
        "test_record_fname = '/content/tensorflow-object-detection-faster-rcnn/data/test/bus.tfrecord'\n",
        "train_record_fname = '/content/tensorflow-object-detection-faster-rcnn/data/train/bus.tfrecord'\n",
        "label_map_pbtxt_fname = '/content/tensorflow-object-detection-faster-rcnn/data/train/bus_label_map.pbtxt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzMxk7UWyS6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91c0c3dd-52fd-43a1-b4c3-d824de957fef"
      },
      "source": [
        "#double check that our class names came through\n",
        "#Rename based on your data above^^\n",
        "%cat '/content/tensorflow-object-detection-faster-rcnn/data/train/bus_label_map.pbtxt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "item {\n",
            "    name: \"busfront\",\n",
            "    id: 1,\n",
            "    display_name: \"busfront\"\n",
            "}\n",
            "item {\n",
            "    name: \"busrear\",\n",
            "    id: 2,\n",
            "    display_name: \"busrear\"\n",
            "}\n",
            "item {\n",
            "    name: \"busroute\",\n",
            "    id: 3,\n",
            "    display_name: \"busroute\"\n",
            "}\n",
            "item {\n",
            "    name: \"busside\",\n",
            "    id: 4,\n",
            "    display_name: \"busside\"\n",
            "}\n",
            "item {\n",
            "    name: \"frontdoor\",\n",
            "    id: 5,\n",
            "    display_name: \"frontdoor\"\n",
            "}\n",
            "item {\n",
            "    name: \"reardoor\",\n",
            "    id: 6,\n",
            "    display_name: \"reardoor\"\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MpYFYzNNumA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366fd540-16cd-40c0-d833-3d221c0d9bc3"
      },
      "source": [
        "!ls '/content/tensorflow-object-detection-faster-rcnn/data/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FYI.txt  README.roboflow.txt  test  train  valid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCNYAaC7w6N8"
      },
      "source": [
        "## Download the Mobilenet SSD v2 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orDCj6ihgUMR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25ea9b9a-e1fe-42bb-c4a3-9ed1d604c28b"
      },
      "source": [
        "%cd /content/models/research\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import urllib.request\n",
        "import tarfile\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "DEST_DIR = '/content/models/research/pretrained_model'\n",
        "\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)\n",
        "!echo {DEST_DIR}\n",
        "!ls -alh {DEST_DIR}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research\n",
            "/content/models/research/pretrained_model\n",
            "total 135M\n",
            "drwxr-xr-x  3 345018 89939 4.0K Mar 30  2018 .\n",
            "drwxr-xr-x 63 root   root  4.0K Jul 29 05:09 ..\n",
            "-rw-r--r--  1 345018 89939   77 Mar 30  2018 checkpoint\n",
            "-rw-r--r--  1 345018 89939  67M Mar 30  2018 frozen_inference_graph.pb\n",
            "-rw-r--r--  1 345018 89939  65M Mar 30  2018 model.ckpt.data-00000-of-00001\n",
            "-rw-r--r--  1 345018 89939  15K Mar 30  2018 model.ckpt.index\n",
            "-rw-r--r--  1 345018 89939 3.4M Mar 30  2018 model.ckpt.meta\n",
            "-rw-r--r--  1 345018 89939 4.2K Mar 30  2018 pipeline.config\n",
            "drwxr-xr-x  3 345018 89939 4.0K Mar 30  2018 saved_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHnxlfRznPP3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fbd7328a-adce-43cd-ee13-1842b0224d8d"
      },
      "source": [
        "#TF pretrained model checkpoint\n",
        "fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n",
        "fine_tune_checkpoint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/models/research/pretrained_model/model.ckpt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvwtHlLOeRJD"
      },
      "source": [
        "## Configuring a Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIhw7IdpLuiU"
      },
      "source": [
        "import os\n",
        "pipeline_fname = os.path.join('/content/models/research/object_detection/samples/configs/', pipeline_file)\n",
        "\n",
        "assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)\n",
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjtCbLF2i0wI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be39c9d6-5817-4d27-edaf-4c6a2c4a6dac"
      },
      "source": [
        "import re\n",
        "iou_threshold = 0.50\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "with open(pipeline_fname) as f:\n",
        "    print(pipeline_fname)\n",
        "    s = f.read()\n",
        "with open(pipeline_fname, 'w') as f:\n",
        "    \n",
        "    # fine_tune_checkpoint\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "    \n",
        "    # tfrecord files train and test.\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n",
        "\n",
        "    # label_map_path\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    # Set training batch_size.\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    # Set training steps, num_steps\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "    \n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "    # Set number of classes num_classes.\n",
        "    s = re.sub('iou_threshold: [0-9].[0-9]+',\n",
        "               'iou_threshold: {}'.format(iou_threshold), s)\n",
        "    \n",
        "    f.write(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection/samples/configs/ssd_mobilenet_v2_coco.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH0MEEanocn6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b94d1e-f7a2-4889-ca6a-cc019887f7cf"
      },
      "source": [
        "# #Have a look at the config file with various settings\n",
        "!cat {pipeline_fname}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# SSD with Mobilenet v2 configuration for MSCOCO Dataset.\n",
            "# Users should configure the fine_tune_checkpoint field in the train config as\n",
            "# well as the label_map_path and input_path fields in the train_input_reader and\n",
            "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
            "# should be configured.\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    num_classes: 6\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    anchor_generator {\n",
            "      ssd_anchor_generator {\n",
            "        num_layers: 6\n",
            "        min_scale: 0.2\n",
            "        max_scale: 0.95\n",
            "        aspect_ratios: 1.0\n",
            "        aspect_ratios: 2.0\n",
            "        aspect_ratios: 0.5\n",
            "        aspect_ratios: 3.0\n",
            "        aspect_ratios: 0.3333\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 300\n",
            "        width: 300\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      convolutional_box_predictor {\n",
            "        min_depth: 0\n",
            "        max_depth: 0\n",
            "        num_layers_before_predictor: 0\n",
            "        use_dropout: false\n",
            "        dropout_keep_probability: 0.8\n",
            "        kernel_size: 1\n",
            "        box_code_size: 4\n",
            "        apply_sigmoid_to_scores: false\n",
            "        conv_hyperparams {\n",
            "          activation: RELU_6,\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.00004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            truncated_normal_initializer {\n",
            "              stddev: 0.03\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            train: true,\n",
            "            scale: true,\n",
            "            center: true,\n",
            "            decay: 0.9997,\n",
            "            epsilon: 0.001,\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_mobilenet_v2'\n",
            "      min_depth: 16\n",
            "      depth_multiplier: 1.0\n",
            "      conv_hyperparams {\n",
            "        activation: RELU_6,\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.00004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            stddev: 0.03\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          train: true,\n",
            "          scale: true,\n",
            "          center: true,\n",
            "          decay: 0.9997,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid {\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      hard_example_miner {\n",
            "        num_hard_examples: 3000\n",
            "        iou_threshold: 0.5\n",
            "        loss_type: CLASSIFICATION\n",
            "        max_negatives_per_positive: 3\n",
            "        min_negatives_per_image: 3\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.5\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  batch_size: 24\n",
            "  optimizer {\n",
            "    rms_prop_optimizer: {\n",
            "      learning_rate: {\n",
            "        exponential_decay_learning_rate {\n",
            "          initial_learning_rate: 0.004\n",
            "          decay_steps: 800720\n",
            "          decay_factor: 0.95\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "      decay: 0.9\n",
            "      epsilon: 1.0\n",
            "    }\n",
            "  }\n",
            "  fine_tune_checkpoint: \"/content/models/research/pretrained_model/model.ckpt\"\n",
            "  fine_tune_checkpoint_type:  \"detection\"\n",
            "  # Note: The below line limits the training process to 200K steps, which we\n",
            "  # empirically found to be sufficient enough to train the pets dataset. This\n",
            "  # effectively bypasses the learning rate schedule (the learning rate will\n",
            "  # never decay). Remove the below line to train indefinitely.\n",
            "  num_steps: 3000\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    ssd_random_crop {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/tensorflow-object-detection-faster-rcnn/data/train/bus.tfrecord\"\n",
            "  }\n",
            "  label_map_path: \"/content/tensorflow-object-detection-faster-rcnn/data/train/bus_label_map.pbtxt\"\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  num_examples: 8000\n",
            "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
            "  # Remove the below line to evaluate indefinitely.\n",
            "  max_evals: 10\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/tensorflow-object-detection-faster-rcnn/data/test/bus.tfrecord\"\n",
            "  }\n",
            "  label_map_path: \"/content/tensorflow-object-detection-faster-rcnn/data/train/bus_label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_readers: 1\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDddx2rPfex9"
      },
      "source": [
        "# Train Custom Mobilenet Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjDHjhKQofT5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38145f85-fa39-44f1-80b7-02a542cc8636"
      },
      "source": [
        "model_dir = 'training/'\n",
        "# Optionally remove content in output model directory for a fresh start.\n",
        "# !rm -rf {model_dir}\n",
        "# os.makedirs(model_dir, exist_ok=True)\n",
        "!python /content/models/research/object_detection/model_main.py \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --num_eval_steps={num_eval_steps}\n",
        "\n",
        "#training will take a while (the TF OD library is not heavily optimized for speed on GPU), watch the mAP metrics rise, you can quit training early when you think your model has maxed out performance"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
            "W0729 05:09:28.074752 140170885773184 model_lib.py:717] Forced number of epochs for all eval validations to be 1.\n",
            "INFO:tensorflow:Maybe overwriting train_steps: 3000\n",
            "I0729 05:09:28.075085 140170885773184 config_util.py:552] Maybe overwriting train_steps: 3000\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0729 05:09:28.075299 140170885773184 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "I0729 05:09:28.075467 140170885773184 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
            "I0729 05:09:28.075664 140170885773184 config_util.py:552] Maybe overwriting eval_num_epochs: 1\n",
            "INFO:tensorflow:Maybe overwriting load_pretrained: True\n",
            "I0729 05:09:28.075836 140170885773184 config_util.py:552] Maybe overwriting load_pretrained: True\n",
            "INFO:tensorflow:Ignoring config override key: load_pretrained\n",
            "I0729 05:09:28.075994 140170885773184 config_util.py:562] Ignoring config override key: load_pretrained\n",
            "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "W0729 05:09:28.076320 140170885773184 model_lib.py:733] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
            "I0729 05:09:28.076485 140170885773184 model_lib.py:768] create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7ba7220450>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "I0729 05:09:28.077013 140170885773184 estimator.py:212] Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7ba7220450>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f7ba780ff80>) includes params argument, but params are not passed to Estimator.\n",
            "W0729 05:09:28.077395 140170885773184 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f7ba780ff80>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "I0729 05:09:28.078017 140170885773184 estimator_training.py:186] Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "I0729 05:09:28.078324 140170885773184 training.py:612] Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "I0729 05:09:28.078764 140170885773184 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0729 05:09:28.094253 140170885773184 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0729 05:09:28.122237 140170885773184 dataset_builder.py:83] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0729 05:09:28.128156 140170885773184 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0729 05:09:28.151506 140170885773184 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7ba7815850>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0729 05:09:28.186183 140170885773184 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7ba7815850>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7f7ba7944440> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "W0729 05:09:28.423256 140170885773184 ag_logging.py:146] Entity <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7f7ba7944440> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:79: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W0729 05:09:28.435434 140170885773184 deprecation.py:323] From /content/models/research/object_detection/inputs.py:79: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0729 05:09:28.444925 140170885773184 deprecation.py:323] From /content/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/core/preprocessor.py:199: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0729 05:09:28.562045 140170885773184 deprecation.py:323] From /content/models/research/object_detection/core/preprocessor.py:199: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:260: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0729 05:09:29.488814 140170885773184 deprecation.py:323] From /content/models/research/object_detection/inputs.py:260: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0729 05:09:30.031968 140170885773184 estimator.py:1148] Calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W0729 05:09:30.630222 140170885773184 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:09:33.791994 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:09:33.832579 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:09:33.869616 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:09:33.907766 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:09:33.943999 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:09:33.982621 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "W0729 05:09:34.030918 140170885773184 variables_helper.py:153] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 512]], model variable shape: [[3, 3, 256, 512]]. This variable will not be initialized from the checkpoint.\n",
            "W0729 05:09:34.031117 140170885773184 variables_helper.py:153] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 256]], model variable shape: [[3, 3, 128, 256]]. This variable will not be initialized from the checkpoint.\n",
            "W0729 05:09:34.031295 140170885773184 variables_helper.py:153] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 256]], model variable shape: [[3, 3, 128, 256]]. This variable will not be initialized from the checkpoint.\n",
            "W0729 05:09:34.031477 140170885773184 variables_helper.py:153] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 64, 128]], model variable shape: [[3, 3, 64, 128]]. This variable will not be initialized from the checkpoint.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0729 05:09:42.967467 140170885773184 deprecation.py:506] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0729 05:09:50.595577 140170885773184 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "I0729 05:09:50.597122 140170885773184 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0729 05:09:55.145337 140170885773184 monitored_session.py:240] Graph was finalized.\n",
            "2021-07-29 05:09:55.156644: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
            "2021-07-29 05:09:55.157016: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5652062b6bc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-07-29 05:09:55.157053: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2021-07-29 05:09:55.162091: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2021-07-29 05:09:55.355872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:09:55.356931: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5652163721c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2021-07-29 05:09:55.356971: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2021-07-29 05:09:55.358382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:09:55.360029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-29 05:09:55.405303: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-29 05:09:55.920083: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-29 05:09:56.098287: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-29 05:09:56.121858: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-29 05:09:56.392474: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-29 05:09:56.420254: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-29 05:09:56.916676: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-29 05:09:56.916946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:09:56.917835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:09:56.918537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-29 05:09:56.922198: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-29 05:09:56.923964: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-29 05:09:56.923999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-29 05:09:56.924017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-29 05:09:56.925260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:09:56.926161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:09:56.926932: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-07-29 05:09:56.926995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10813 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0729 05:10:07.947834 140170885773184 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0729 05:10:08.402635 140170885773184 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into training/model.ckpt.\n",
            "I0729 05:10:21.275097 140170885773184 basic_session_run_hooks.py:606] Saving checkpoints for 0 into training/model.ckpt.\n",
            "2021-07-29 05:10:33.819511: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-29 05:10:37.866208: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "INFO:tensorflow:loss = 7.660645, step = 0\n",
            "I0729 05:10:41.917154 140170885773184 basic_session_run_hooks.py:262] loss = 7.660645, step = 0\n",
            "INFO:tensorflow:global_step/sec: 1.2681\n",
            "I0729 05:12:00.774348 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.2681\n",
            "INFO:tensorflow:loss = 4.6739597, step = 100 (78.859 sec)\n",
            "I0729 05:12:00.775886 140170885773184 basic_session_run_hooks.py:260] loss = 4.6739597, step = 100 (78.859 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.39589\n",
            "I0729 05:13:12.413323 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.39589\n",
            "INFO:tensorflow:loss = 2.685008, step = 200 (71.639 sec)\n",
            "I0729 05:13:12.414478 140170885773184 basic_session_run_hooks.py:260] loss = 2.685008, step = 200 (71.639 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.37336\n",
            "I0729 05:14:25.227288 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.37336\n",
            "INFO:tensorflow:loss = 2.9440453, step = 300 (72.814 sec)\n",
            "I0729 05:14:25.228729 140170885773184 basic_session_run_hooks.py:260] loss = 2.9440453, step = 300 (72.814 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.40323\n",
            "I0729 05:15:36.491302 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.40323\n",
            "INFO:tensorflow:loss = 2.326732, step = 400 (71.264 sec)\n",
            "I0729 05:15:36.492610 140170885773184 basic_session_run_hooks.py:260] loss = 2.326732, step = 400 (71.264 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.38399\n",
            "I0729 05:16:48.745882 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.38399\n",
            "INFO:tensorflow:loss = 2.2826376, step = 500 (72.255 sec)\n",
            "I0729 05:16:48.747320 140170885773184 basic_session_run_hooks.py:260] loss = 2.2826376, step = 500 (72.255 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.39075\n",
            "I0729 05:18:00.649507 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.39075\n",
            "INFO:tensorflow:loss = 2.0858865, step = 600 (71.903 sec)\n",
            "I0729 05:18:00.650789 140170885773184 basic_session_run_hooks.py:260] loss = 2.0858865, step = 600 (71.903 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.37926\n",
            "I0729 05:19:13.152253 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.37926\n",
            "INFO:tensorflow:loss = 2.0491552, step = 700 (72.503 sec)\n",
            "I0729 05:19:13.153543 140170885773184 basic_session_run_hooks.py:260] loss = 2.0491552, step = 700 (72.503 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 801 into training/model.ckpt.\n",
            "I0729 05:20:24.770686 140170885773184 basic_session_run_hooks.py:606] Saving checkpoints for 801 into training/model.ckpt.\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7b910c49d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0729 05:20:26.737462 140170885773184 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7b910c49d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7b9ef85710> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "W0729 05:20:26.972030 140170885773184 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7b9ef85710> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0729 05:20:27.594719 140170885773184 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:20:30.464423 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:20:30.501641 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:20:30.539670 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:20:30.575554 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:20:30.612517 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:20:30.647982 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/eval_util.py:830: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0729 05:20:31.657202 140170885773184 deprecation.py:323] From /content/models/research/object_detection/eval_util.py:830: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/models/research/object_detection/utils/visualization_utils.py:618: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0729 05:20:31.891745 140170885773184 deprecation.py:323] From /content/models/research/object_detection/utils/visualization_utils.py:618: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0729 05:20:32.515123 140170885773184 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-07-29T05:20:32Z\n",
            "I0729 05:20:32.534938 140170885773184 evaluation.py:255] Starting evaluation at 2021-07-29T05:20:32Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0729 05:20:33.051801 140170885773184 monitored_session.py:240] Graph was finalized.\n",
            "2021-07-29 05:20:33.053038: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:20:33.053660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-29 05:20:33.053811: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-29 05:20:33.053855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-29 05:20:33.053904: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-29 05:20:33.053951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-29 05:20:33.053991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-29 05:20:33.054032: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-29 05:20:33.054100: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-29 05:20:33.054213: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:20:33.054857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:20:33.055418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-29 05:20:33.055476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-29 05:20:33.055497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-29 05:20:33.055511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-29 05:20:33.055656: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:20:33.056298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:20:33.056848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10813 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-801\n",
            "I0729 05:20:33.057927 140170885773184 saver.py:1284] Restoring parameters from training/model.ckpt-801\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0729 05:20:33.972208 140170885773184 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0729 05:20:34.104016 140170885773184 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 72 images.\n",
            "I0729 05:20:39.683373 140168708093696 coco_evaluation.py:237] Performing evaluation on 72 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0729 05:20:39.684251 140168708093696 coco_tools.py:116] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.01s)\n",
            "I0729 05:20:39.691250 140168708093696 coco_tools.py:138] DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.56s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.19s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.059\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.183\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.014\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.026\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.120\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.107\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.164\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.173\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.090\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.323\n",
            "INFO:tensorflow:Finished evaluation at 2021-07-29-05:20:40\n",
            "I0729 05:20:40.645288 140170885773184 evaluation.py:275] Finished evaluation at 2021-07-29-05:20:40\n",
            "INFO:tensorflow:Saving dict for global step 801: DetectionBoxes_Precision/mAP = 0.058778003, DetectionBoxes_Precision/mAP (large) = 0.120232575, DetectionBoxes_Precision/mAP (medium) = 0.026376335, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.18341775, DetectionBoxes_Precision/mAP@.75IOU = 0.013793593, DetectionBoxes_Recall/AR@1 = 0.1069026, DetectionBoxes_Recall/AR@10 = 0.16414656, DetectionBoxes_Recall/AR@100 = 0.17336962, DetectionBoxes_Recall/AR@100 (large) = 0.32282943, DetectionBoxes_Recall/AR@100 (medium) = 0.09045966, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 2.5814297, Loss/localization_loss = 1.0640574, Loss/regularization_loss = 0.24808991, Loss/total_loss = 3.8935764, global_step = 801, learning_rate = 0.004, loss = 3.8935764\n",
            "I0729 05:20:40.645633 140170885773184 estimator.py:2049] Saving dict for global step 801: DetectionBoxes_Precision/mAP = 0.058778003, DetectionBoxes_Precision/mAP (large) = 0.120232575, DetectionBoxes_Precision/mAP (medium) = 0.026376335, DetectionBoxes_Precision/mAP (small) = 0.0, DetectionBoxes_Precision/mAP@.50IOU = 0.18341775, DetectionBoxes_Precision/mAP@.75IOU = 0.013793593, DetectionBoxes_Recall/AR@1 = 0.1069026, DetectionBoxes_Recall/AR@10 = 0.16414656, DetectionBoxes_Recall/AR@100 = 0.17336962, DetectionBoxes_Recall/AR@100 (large) = 0.32282943, DetectionBoxes_Recall/AR@100 (medium) = 0.09045966, DetectionBoxes_Recall/AR@100 (small) = 0.0, Loss/classification_loss = 2.5814297, Loss/localization_loss = 1.0640574, Loss/regularization_loss = 0.24808991, Loss/total_loss = 3.8935764, global_step = 801, learning_rate = 0.004, loss = 3.8935764\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 801: training/model.ckpt-801\n",
            "I0729 05:20:41.527737 140170885773184 estimator.py:2109] Saving 'checkpoint_path' summary for global step 801: training/model.ckpt-801\n",
            "INFO:tensorflow:global_step/sec: 1.13151\n",
            "I0729 05:20:41.529506 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.13151\n",
            "INFO:tensorflow:loss = 2.0620036, step = 800 (88.377 sec)\n",
            "I0729 05:20:41.530494 140170885773184 basic_session_run_hooks.py:260] loss = 2.0620036, step = 800 (88.377 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.40386\n",
            "I0729 05:21:52.761555 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.40386\n",
            "INFO:tensorflow:loss = 2.2869985, step = 900 (71.232 sec)\n",
            "I0729 05:21:52.762840 140170885773184 basic_session_run_hooks.py:260] loss = 2.2869985, step = 900 (71.232 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.37395\n",
            "I0729 05:23:05.544690 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.37395\n",
            "INFO:tensorflow:loss = 1.8416421, step = 1000 (72.783 sec)\n",
            "I0729 05:23:05.546011 140170885773184 basic_session_run_hooks.py:260] loss = 1.8416421, step = 1000 (72.783 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.39395\n",
            "I0729 05:24:17.283089 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.39395\n",
            "INFO:tensorflow:loss = 1.654398, step = 1100 (71.739 sec)\n",
            "I0729 05:24:17.284680 140170885773184 basic_session_run_hooks.py:260] loss = 1.654398, step = 1100 (71.739 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.38148\n",
            "I0729 05:25:29.669231 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.38148\n",
            "INFO:tensorflow:loss = 1.8908613, step = 1200 (72.386 sec)\n",
            "I0729 05:25:29.670247 140170885773184 basic_session_run_hooks.py:260] loss = 1.8908613, step = 1200 (72.386 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.40204\n",
            "I0729 05:26:40.993607 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.40204\n",
            "INFO:tensorflow:loss = 1.7529597, step = 1300 (71.325 sec)\n",
            "I0729 05:26:40.995144 140170885773184 basic_session_run_hooks.py:260] loss = 1.7529597, step = 1300 (71.325 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.38205\n",
            "I0729 05:27:53.350199 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.38205\n",
            "INFO:tensorflow:loss = 1.4722838, step = 1400 (72.356 sec)\n",
            "I0729 05:27:53.351576 140170885773184 basic_session_run_hooks.py:260] loss = 1.4722838, step = 1400 (72.356 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.42319\n",
            "I0729 05:29:03.614936 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.42319\n",
            "INFO:tensorflow:loss = 1.5645382, step = 1500 (70.265 sec)\n",
            "I0729 05:29:03.616405 140170885773184 basic_session_run_hooks.py:260] loss = 1.5645382, step = 1500 (70.265 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.40919\n",
            "I0729 05:30:14.577501 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.40919\n",
            "INFO:tensorflow:loss = 1.4782146, step = 1600 (70.962 sec)\n",
            "I0729 05:30:14.578707 140170885773184 basic_session_run_hooks.py:260] loss = 1.4782146, step = 1600 (70.962 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1616 into training/model.ckpt.\n",
            "I0729 05:30:25.106375 140170885773184 basic_session_run_hooks.py:606] Saving checkpoints for 1616 into training/model.ckpt.\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7b442fcad0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0729 05:30:26.992988 140170885773184 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7b442fcad0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7b3e714050> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "W0729 05:30:27.213734 140170885773184 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7b3e714050> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0729 05:30:27.821284 140170885773184 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:30:30.502137 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:30:30.540411 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:30:30.577691 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:30:30.614022 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:30:30.653099 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:30:30.689074 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0729 05:30:32.903489 140170885773184 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-07-29T05:30:32Z\n",
            "I0729 05:30:32.923784 140170885773184 evaluation.py:255] Starting evaluation at 2021-07-29T05:30:32Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0729 05:30:33.442680 140170885773184 monitored_session.py:240] Graph was finalized.\n",
            "2021-07-29 05:30:33.443469: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:30:33.444130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-29 05:30:33.444251: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-29 05:30:33.444298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-29 05:30:33.444355: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-29 05:30:33.444397: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-29 05:30:33.444438: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-29 05:30:33.444484: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-29 05:30:33.444529: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-29 05:30:33.444623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:30:33.445248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:30:33.445745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-29 05:30:33.445816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-29 05:30:33.445837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-29 05:30:33.445852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-29 05:30:33.445991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:30:33.446549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:30:33.447102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10813 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-1616\n",
            "I0729 05:30:33.448297 140170885773184 saver.py:1284] Restoring parameters from training/model.ckpt-1616\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0729 05:30:34.377419 140170885773184 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0729 05:30:34.520330 140170885773184 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 72 images.\n",
            "I0729 05:30:39.689045 140168716486400 coco_evaluation.py:237] Performing evaluation on 72 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0729 05:30:39.689890 140168716486400 coco_tools.py:116] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.00s)\n",
            "I0729 05:30:39.693389 140168716486400 coco_tools.py:138] DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.53s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.20s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.080\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.214\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.034\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.012\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.041\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.155\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.124\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.189\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.196\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.012\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.136\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.353\n",
            "INFO:tensorflow:Finished evaluation at 2021-07-29-05:30:40\n",
            "I0729 05:30:40.599415 140170885773184 evaluation.py:275] Finished evaluation at 2021-07-29-05:30:40\n",
            "INFO:tensorflow:Saving dict for global step 1616: DetectionBoxes_Precision/mAP = 0.08007685, DetectionBoxes_Precision/mAP (large) = 0.15454897, DetectionBoxes_Precision/mAP (medium) = 0.041347366, DetectionBoxes_Precision/mAP (small) = 0.012277228, DetectionBoxes_Precision/mAP@.50IOU = 0.21369702, DetectionBoxes_Precision/mAP@.75IOU = 0.034324493, DetectionBoxes_Recall/AR@1 = 0.123944, DetectionBoxes_Recall/AR@10 = 0.18881033, DetectionBoxes_Recall/AR@100 = 0.19605343, DetectionBoxes_Recall/AR@100 (large) = 0.35266098, DetectionBoxes_Recall/AR@100 (medium) = 0.13564815, DetectionBoxes_Recall/AR@100 (small) = 0.012, Loss/classification_loss = 1.9259222, Loss/localization_loss = 0.7595525, Loss/regularization_loss = 0.2476548, Loss/total_loss = 2.933129, global_step = 1616, learning_rate = 0.004, loss = 2.933129\n",
            "I0729 05:30:40.599772 140170885773184 estimator.py:2049] Saving dict for global step 1616: DetectionBoxes_Precision/mAP = 0.08007685, DetectionBoxes_Precision/mAP (large) = 0.15454897, DetectionBoxes_Precision/mAP (medium) = 0.041347366, DetectionBoxes_Precision/mAP (small) = 0.012277228, DetectionBoxes_Precision/mAP@.50IOU = 0.21369702, DetectionBoxes_Precision/mAP@.75IOU = 0.034324493, DetectionBoxes_Recall/AR@1 = 0.123944, DetectionBoxes_Recall/AR@10 = 0.18881033, DetectionBoxes_Recall/AR@100 = 0.19605343, DetectionBoxes_Recall/AR@100 (large) = 0.35266098, DetectionBoxes_Recall/AR@100 (medium) = 0.13564815, DetectionBoxes_Recall/AR@100 (small) = 0.012, Loss/classification_loss = 1.9259222, Loss/localization_loss = 0.7595525, Loss/regularization_loss = 0.2476548, Loss/total_loss = 2.933129, global_step = 1616, learning_rate = 0.004, loss = 2.933129\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1616: training/model.ckpt-1616\n",
            "I0729 05:30:40.603688 140170885773184 estimator.py:2109] Saving 'checkpoint_path' summary for global step 1616: training/model.ckpt-1616\n",
            "INFO:tensorflow:global_step/sec: 1.16588\n",
            "I0729 05:31:40.349562 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.16588\n",
            "INFO:tensorflow:loss = 1.4563833, step = 1700 (85.772 sec)\n",
            "I0729 05:31:40.350777 140170885773184 basic_session_run_hooks.py:260] loss = 1.4563833, step = 1700 (85.772 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.40814\n",
            "I0729 05:32:51.365378 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.40814\n",
            "INFO:tensorflow:loss = 1.518697, step = 1800 (71.016 sec)\n",
            "I0729 05:32:51.366629 140170885773184 basic_session_run_hooks.py:260] loss = 1.518697, step = 1800 (71.016 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.41819\n",
            "I0729 05:34:01.877939 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.41819\n",
            "INFO:tensorflow:loss = 1.2126728, step = 1900 (70.512 sec)\n",
            "I0729 05:34:01.879095 140170885773184 basic_session_run_hooks.py:260] loss = 1.2126728, step = 1900 (70.512 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.39965\n",
            "I0729 05:35:13.324524 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.39965\n",
            "INFO:tensorflow:loss = 1.6813498, step = 2000 (71.447 sec)\n",
            "I0729 05:35:13.326132 140170885773184 basic_session_run_hooks.py:260] loss = 1.6813498, step = 2000 (71.447 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.42118\n",
            "I0729 05:36:23.688670 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.42118\n",
            "INFO:tensorflow:loss = 1.6895661, step = 2100 (70.364 sec)\n",
            "I0729 05:36:23.689953 140170885773184 basic_session_run_hooks.py:260] loss = 1.6895661, step = 2100 (70.364 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.40175\n",
            "I0729 05:37:35.027866 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.40175\n",
            "INFO:tensorflow:loss = 1.9365394, step = 2200 (71.339 sec)\n",
            "I0729 05:37:35.029047 140170885773184 basic_session_run_hooks.py:260] loss = 1.9365394, step = 2200 (71.339 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.42331\n",
            "I0729 05:38:45.286853 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.42331\n",
            "INFO:tensorflow:loss = 1.2207475, step = 2300 (70.259 sec)\n",
            "I0729 05:38:45.288099 140170885773184 basic_session_run_hooks.py:260] loss = 1.2207475, step = 2300 (70.259 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.39992\n",
            "I0729 05:39:56.719792 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.39992\n",
            "INFO:tensorflow:loss = 1.4595674, step = 2400 (71.433 sec)\n",
            "I0729 05:39:56.721010 140170885773184 basic_session_run_hooks.py:260] loss = 1.4595674, step = 2400 (71.433 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 2442 into training/model.ckpt.\n",
            "I0729 05:40:25.521922 140170885773184 basic_session_run_hooks.py:606] Saving checkpoints for 2442 into training/model.ckpt.\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7b90f9c5d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0729 05:40:27.337117 140170885773184 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7b90f9c5d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7b44feeef0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "W0729 05:40:27.526801 140170885773184 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7b44feeef0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0729 05:40:28.084313 140170885773184 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:40:30.579820 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:40:30.616547 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:40:30.652455 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:40:30.687970 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:40:30.722703 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:40:30.758572 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0729 05:40:32.532269 140170885773184 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-07-29T05:40:32Z\n",
            "I0729 05:40:32.552744 140170885773184 evaluation.py:255] Starting evaluation at 2021-07-29T05:40:32Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0729 05:40:33.040792 140170885773184 monitored_session.py:240] Graph was finalized.\n",
            "2021-07-29 05:40:33.041518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:40:33.042124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-29 05:40:33.042274: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-29 05:40:33.042323: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-29 05:40:33.042372: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-29 05:40:33.042423: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-29 05:40:33.042496: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-29 05:40:33.042545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-29 05:40:33.042588: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-29 05:40:33.042674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:40:33.043244: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:40:33.043706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-29 05:40:33.043792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-29 05:40:33.043829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-29 05:40:33.043843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-29 05:40:33.043975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:40:33.044505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:40:33.045029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10813 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-2442\n",
            "I0729 05:40:33.046010 140170885773184 saver.py:1284] Restoring parameters from training/model.ckpt-2442\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0729 05:40:33.932203 140170885773184 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0729 05:40:34.052332 140170885773184 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 72 images.\n",
            "I0729 05:40:39.063123 140168708093696 coco_evaluation.py:237] Performing evaluation on 72 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0729 05:40:39.063845 140168708093696 coco_tools.py:116] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.00s)\n",
            "I0729 05:40:39.067876 140168708093696 coco_tools.py:138] DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.54s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.18s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.092\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.249\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.035\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.021\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.054\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.172\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.128\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.199\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.206\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.020\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.175\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.323\n",
            "INFO:tensorflow:Finished evaluation at 2021-07-29-05:40:39\n",
            "I0729 05:40:39.951347 140170885773184 evaluation.py:275] Finished evaluation at 2021-07-29-05:40:39\n",
            "INFO:tensorflow:Saving dict for global step 2442: DetectionBoxes_Precision/mAP = 0.09223848, DetectionBoxes_Precision/mAP (large) = 0.17234851, DetectionBoxes_Precision/mAP (medium) = 0.053924266, DetectionBoxes_Precision/mAP (small) = 0.020594059, DetectionBoxes_Precision/mAP@.50IOU = 0.2491785, DetectionBoxes_Precision/mAP@.75IOU = 0.034637913, DetectionBoxes_Recall/AR@1 = 0.12765305, DetectionBoxes_Recall/AR@10 = 0.19890156, DetectionBoxes_Recall/AR@100 = 0.20586349, DetectionBoxes_Recall/AR@100 (large) = 0.32256114, DetectionBoxes_Recall/AR@100 (medium) = 0.17516534, DetectionBoxes_Recall/AR@100 (small) = 0.02, Loss/classification_loss = 1.518391, Loss/localization_loss = 0.5510709, Loss/regularization_loss = 0.24716489, Loss/total_loss = 2.3166268, global_step = 2442, learning_rate = 0.004, loss = 2.3166268\n",
            "I0729 05:40:39.951703 140170885773184 estimator.py:2049] Saving dict for global step 2442: DetectionBoxes_Precision/mAP = 0.09223848, DetectionBoxes_Precision/mAP (large) = 0.17234851, DetectionBoxes_Precision/mAP (medium) = 0.053924266, DetectionBoxes_Precision/mAP (small) = 0.020594059, DetectionBoxes_Precision/mAP@.50IOU = 0.2491785, DetectionBoxes_Precision/mAP@.75IOU = 0.034637913, DetectionBoxes_Recall/AR@1 = 0.12765305, DetectionBoxes_Recall/AR@10 = 0.19890156, DetectionBoxes_Recall/AR@100 = 0.20586349, DetectionBoxes_Recall/AR@100 (large) = 0.32256114, DetectionBoxes_Recall/AR@100 (medium) = 0.17516534, DetectionBoxes_Recall/AR@100 (small) = 0.02, Loss/classification_loss = 1.518391, Loss/localization_loss = 0.5510709, Loss/regularization_loss = 0.24716489, Loss/total_loss = 2.3166268, global_step = 2442, learning_rate = 0.004, loss = 2.3166268\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2442: training/model.ckpt-2442\n",
            "I0729 05:40:39.955223 140170885773184 estimator.py:2109] Saving 'checkpoint_path' summary for global step 2442: training/model.ckpt-2442\n",
            "INFO:tensorflow:global_step/sec: 1.17405\n",
            "I0729 05:41:21.894987 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.17405\n",
            "INFO:tensorflow:loss = 1.4114355, step = 2500 (85.175 sec)\n",
            "I0729 05:41:21.896486 140170885773184 basic_session_run_hooks.py:260] loss = 1.4114355, step = 2500 (85.175 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.42303\n",
            "I0729 05:42:32.167461 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.42303\n",
            "INFO:tensorflow:loss = 1.5912925, step = 2600 (70.272 sec)\n",
            "I0729 05:42:32.168650 140170885773184 basic_session_run_hooks.py:260] loss = 1.5912925, step = 2600 (70.272 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.41653\n",
            "I0729 05:43:42.762406 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.41653\n",
            "INFO:tensorflow:loss = 1.2939831, step = 2700 (70.595 sec)\n",
            "I0729 05:43:42.763648 140170885773184 basic_session_run_hooks.py:260] loss = 1.2939831, step = 2700 (70.595 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.42708\n",
            "I0729 05:44:52.835425 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.42708\n",
            "INFO:tensorflow:loss = 1.2557038, step = 2800 (70.073 sec)\n",
            "I0729 05:44:52.836832 140170885773184 basic_session_run_hooks.py:260] loss = 1.2557038, step = 2800 (70.073 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.4222\n",
            "I0729 05:46:03.149152 140170885773184 basic_session_run_hooks.py:692] global_step/sec: 1.4222\n",
            "INFO:tensorflow:loss = 1.5142578, step = 2900 (70.314 sec)\n",
            "I0729 05:46:03.150486 140170885773184 basic_session_run_hooks.py:260] loss = 1.5142578, step = 2900 (70.314 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 3000 into training/model.ckpt.\n",
            "I0729 05:47:12.441918 140170885773184 basic_session_run_hooks.py:606] Saving checkpoints for 3000 into training/model.ckpt.\n",
            "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "I0729 05:47:14.164595 140170885773184 training.py:527] Skip the current checkpoint eval due to throttle secs (600 secs).\n",
            "WARNING:tensorflow:Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7b407b8050>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "W0729 05:47:14.232402 140170885773184 ag_logging.py:146] Entity <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f7b407b8050>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
            "WARNING:tensorflow:Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7b407994d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "W0729 05:47:14.428726 140170885773184 ag_logging.py:146] Entity <function eval_input.<locals>.transform_and_pad_input_data_fn at 0x7f7b407994d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0729 05:47:15.000886 140170885773184 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:47:17.918623 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:47:17.955835 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:47:17.990083 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:47:18.024223 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:47:18.058252 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:47:18.090237 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0729 05:47:19.851401 140170885773184 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2021-07-29T05:47:19Z\n",
            "I0729 05:47:19.871161 140170885773184 evaluation.py:255] Starting evaluation at 2021-07-29T05:47:19Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0729 05:47:20.385141 140170885773184 monitored_session.py:240] Graph was finalized.\n",
            "2021-07-29 05:47:20.385978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:47:20.386788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-29 05:47:20.386876: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-29 05:47:20.386919: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-29 05:47:20.386964: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-29 05:47:20.387011: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-29 05:47:20.387061: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-29 05:47:20.387100: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-29 05:47:20.387139: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-29 05:47:20.387262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:47:20.387934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:47:20.388547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-29 05:47:20.388605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-29 05:47:20.388649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-29 05:47:20.388670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-29 05:47:20.388875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:47:20.389517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:47:20.390113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10813 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-3000\n",
            "I0729 05:47:20.391356 140170885773184 saver.py:1284] Restoring parameters from training/model.ckpt-3000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0729 05:47:21.324493 140170885773184 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0729 05:47:21.459857 140170885773184 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 72 images.\n",
            "I0729 05:47:26.674289 140168716486400 coco_evaluation.py:237] Performing evaluation on 72 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0729 05:47:26.675123 140168716486400 coco_tools.py:116] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.00s)\n",
            "I0729 05:47:26.678422 140168716486400 coco_tools.py:138] DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.58s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.19s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.101\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.250\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.040\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.023\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.061\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.184\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.138\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.216\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.220\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.024\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.180\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.374\n",
            "INFO:tensorflow:Finished evaluation at 2021-07-29-05:47:27\n",
            "I0729 05:47:27.631023 140170885773184 evaluation.py:275] Finished evaluation at 2021-07-29-05:47:27\n",
            "INFO:tensorflow:Saving dict for global step 3000: DetectionBoxes_Precision/mAP = 0.101407096, DetectionBoxes_Precision/mAP (large) = 0.18419552, DetectionBoxes_Precision/mAP (medium) = 0.060764816, DetectionBoxes_Precision/mAP (small) = 0.022772277, DetectionBoxes_Precision/mAP@.50IOU = 0.24992082, DetectionBoxes_Precision/mAP@.75IOU = 0.039678387, DetectionBoxes_Recall/AR@1 = 0.13829346, DetectionBoxes_Recall/AR@10 = 0.21591337, DetectionBoxes_Recall/AR@100 = 0.220362, DetectionBoxes_Recall/AR@100 (large) = 0.37409616, DetectionBoxes_Recall/AR@100 (medium) = 0.18002646, DetectionBoxes_Recall/AR@100 (small) = 0.024, Loss/classification_loss = 1.417975, Loss/localization_loss = 0.46114382, Loss/regularization_loss = 0.24682406, Loss/total_loss = 2.1259425, global_step = 3000, learning_rate = 0.004, loss = 2.1259425\n",
            "I0729 05:47:27.631370 140170885773184 estimator.py:2049] Saving dict for global step 3000: DetectionBoxes_Precision/mAP = 0.101407096, DetectionBoxes_Precision/mAP (large) = 0.18419552, DetectionBoxes_Precision/mAP (medium) = 0.060764816, DetectionBoxes_Precision/mAP (small) = 0.022772277, DetectionBoxes_Precision/mAP@.50IOU = 0.24992082, DetectionBoxes_Precision/mAP@.75IOU = 0.039678387, DetectionBoxes_Recall/AR@1 = 0.13829346, DetectionBoxes_Recall/AR@10 = 0.21591337, DetectionBoxes_Recall/AR@100 = 0.220362, DetectionBoxes_Recall/AR@100 (large) = 0.37409616, DetectionBoxes_Recall/AR@100 (medium) = 0.18002646, DetectionBoxes_Recall/AR@100 (small) = 0.024, Loss/classification_loss = 1.417975, Loss/localization_loss = 0.46114382, Loss/regularization_loss = 0.24682406, Loss/total_loss = 2.1259425, global_step = 3000, learning_rate = 0.004, loss = 2.1259425\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3000: training/model.ckpt-3000\n",
            "I0729 05:47:27.634914 140170885773184 estimator.py:2109] Saving 'checkpoint_path' summary for global step 3000: training/model.ckpt-3000\n",
            "INFO:tensorflow:Performing the final export in the end of training.\n",
            "I0729 05:47:27.635682 140170885773184 exporter.py:410] Performing the final export in the end of training.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0729 05:47:27.947793 140170885773184 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:47:30.512606 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:47:30.550128 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:47:30.584967 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:47:30.622069 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:47:30.658681 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0729 05:47:30.694988 140170885773184 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0729 05:47:31.671309 140170885773184 estimator.py:1150] Done calling model_fn.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0729 05:47:31.671591 140170885773184 deprecation.py:323] From /tensorflow-1.15.2/python3.7/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "I0729 05:47:31.672267 140170885773184 export_utils.py:170] Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "I0729 05:47:31.672402 140170885773184 export_utils.py:170] Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['tensorflow/serving/predict', 'serving_default']\n",
            "I0729 05:47:31.672533 140170885773184 export_utils.py:170] Signatures INCLUDED in export for Predict: ['tensorflow/serving/predict', 'serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "I0729 05:47:31.672645 140170885773184 export_utils.py:170] Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "I0729 05:47:31.672758 140170885773184 export_utils.py:170] Signatures INCLUDED in export for Eval: None\n",
            "2021-07-29 05:47:31.673344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:47:31.673990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2021-07-29 05:47:31.674093: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "2021-07-29 05:47:31.674133: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
            "2021-07-29 05:47:31.674197: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
            "2021-07-29 05:47:31.674255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
            "2021-07-29 05:47:31.674318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-07-29 05:47:31.674394: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
            "2021-07-29 05:47:31.674439: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2021-07-29 05:47:31.674545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:47:31.675219: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:47:31.675750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1767] Adding visible gpu devices: 0\n",
            "2021-07-29 05:47:31.675797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1180] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-07-29 05:47:31.675817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1186]      0 \n",
            "2021-07-29 05:47:31.675833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1199] 0:   N \n",
            "2021-07-29 05:47:31.675960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:47:31.676522: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-07-29 05:47:31.677034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1325] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10813 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-3000\n",
            "I0729 05:47:31.679873 140170885773184 saver.py:1284] Restoring parameters from training/model.ckpt-3000\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "I0729 05:47:32.149528 140170885773184 builder_impl.py:665] Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "I0729 05:47:32.149834 140170885773184 builder_impl.py:460] No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: training/export/Servo/temp-b'1627537647'/saved_model.pb\n",
            "I0729 05:47:32.919386 140170885773184 builder_impl.py:425] SavedModel written to: training/export/Servo/temp-b'1627537647'/saved_model.pb\n",
            "INFO:tensorflow:Loss for final step: 1.3710798.\n",
            "I0729 05:47:33.340316 140170885773184 estimator.py:371] Loss for final step: 1.3710798.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KP-tUdtnRybs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c05370c6-4b2e-4e74-9633-a35702622c81"
      },
      "source": [
        "#model dir check for the trained model\n",
        "!ls {model_dir}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\n",
            "eval_0\n",
            "events.out.tfevents.1627535392.36ca09b6e874\n",
            "export\n",
            "graph.pbtxt\n",
            "model.ckpt-0.data-00000-of-00001\n",
            "model.ckpt-0.index\n",
            "model.ckpt-0.meta\n",
            "model.ckpt-1616.data-00000-of-00001\n",
            "model.ckpt-1616.index\n",
            "model.ckpt-1616.meta\n",
            "model.ckpt-2442.data-00000-of-00001\n",
            "model.ckpt-2442.index\n",
            "model.ckpt-2442.meta\n",
            "model.ckpt-3000.data-00000-of-00001\n",
            "model.ckpt-3000.index\n",
            "model.ckpt-3000.meta\n",
            "model.ckpt-801.data-00000-of-00001\n",
            "model.ckpt-801.index\n",
            "model.ckpt-801.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmSESMetj1sa"
      },
      "source": [
        "## Export a Trained Inference Graph\n",
        "Once your training job is complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection. This can be done as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZJ_AF5MD3hZ"
      },
      "source": [
        "#clean output_directory if necessary to start fresh:\n",
        "\n",
        "# !rm -rf /content/object_detection_demo/fine_tuned_model/ \n",
        "# os.makedirs('/content/object_detection_demo_flow/fine_tuned_model/', exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHoP90pUyKSq"
      },
      "source": [
        "%%capture\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "output_directory = './fine_tuned_model'\n",
        "# output_directory = '/content/gdrive/My\\ Drive/data/'\n",
        "\n",
        "lst = os.listdir(model_dir)\n",
        "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
        "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
        "last_model = lst[steps.argmax()].replace('.meta', '')\n",
        "\n",
        "last_model_path = os.path.join(model_dir, last_model)\n",
        "print(last_model_path)\n",
        "!python /content/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path={pipeline_fname} \\\n",
        "    --output_directory={output_directory} \\\n",
        "    --trained_checkpoint_prefix={last_model_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usgBZvkz0nqD"
      },
      "source": [
        "#export directory check\n",
        "# !ls {output_directory}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnDo1lonKgFr"
      },
      "source": [
        "import os\n",
        "pb_fname = os.path.join(os.path.abspath(output_directory), \"saved_model/saved_model.pb\")\n",
        "assert os.path.isfile(pb_fname), '`{}` not exist'.format(pb_fname)\n",
        "# !ls -alh {pb_fname}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZHHqjjfMurV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5797ae0d-da7f-4955-e366-81286005083b"
      },
      "source": [
        "#download frozen graph for posterity, you can keep this so you don't have to start over on training\n",
        "from google.colab import files\n",
        "files.download(pb_fname)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5f690cee-5c34-4df5-951d-99c9bdf83aff\", \"saved_model.pb\", 19677999)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xRFbUGYL4ApA",
        "outputId": "e81b7de1-4572-4e21-d0c0-6336e91fcc69"
      },
      "source": [
        "pb_fname"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/models/research/fine_tuned_model/saved_model/saved_model.pb'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz1gX19GlVW7"
      },
      "source": [
        "## Running Inference: Checking what the trained model can detect\n",
        "Test with images in repository `tensorflow-object-detection-faster-rcnn/data/images/final test` directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iydNnHsvz5II"
      },
      "source": [
        "#You will need to import test images into the test folder specified below - In Roboflow you can export raw images in YOLO Darknet format\n",
        "\n",
        "test_folder = '/content/tensorflow-object-detection-faster-rcnn'\n",
        "sample_img = 'https://storage.googleapis.com/roboflow-platform-transforms/D7CAGZafFYZMvKbKibZGc42BUAx2/56694b0cb356d57b9da4914993042958/transformed.jpg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzj9A4e5mj5l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9c20de1-7e72-4ad3-8d59-9eaf8ec85f28"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
        "PATH_TO_CKPT = pb_fname\n",
        "\n",
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = label_map_pbtxt_fname\n",
        "\n",
        "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
        "PATH_TO_TEST_IMAGES_DIR = test_folder + \"/data/test/\"\n",
        "\n",
        "import urllib.request\n",
        "urllib.request.urlretrieve(sample_img, \n",
        "                           PATH_TO_TEST_IMAGES_DIR + \"language.jpg\")\n",
        "\n",
        "assert os.path.isfile(pb_fname)\n",
        "assert os.path.isfile(PATH_TO_LABELS)\n",
        "TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.jpg*\"))\n",
        "assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n",
        "print(TEST_IMAGE_PATHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/tensorflow-object-detection-faster-rcnn/data/test/language.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CG5YUMdg1Po7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee92f77-33f9-408e-8f14-913b951a1269"
      },
      "source": [
        "%cd /content/models/research/object_detection\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "# This is needed to display the images.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import ops as utils_ops\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    PATH_TO_CKPT = '/content/models/research/fine_tuned_model/frozen_inference_graph.pb'\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(\n",
        "    label_map, max_num_classes=num_classes, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "\n",
        "def load_image_into_numpy_array(image):\n",
        "    (im_width, im_height) = image.size\n",
        "    return np.array(image.getdata()).reshape(\n",
        "        (im_height, im_width, 3)).astype(np.uint8)\n",
        "\n",
        "# Size, in inches, of the output images.\n",
        "IMAGE_SIZE = (12, 8)\n",
        "\n",
        "\n",
        "def run_inference_for_single_image(image, graph):\n",
        "    with graph.as_default():\n",
        "        with tf.Session() as sess:\n",
        "            # Get handles to input and output tensors\n",
        "            ops = tf.get_default_graph().get_operations()\n",
        "            all_tensor_names = {\n",
        "                output.name for op in ops for output in op.outputs}\n",
        "            tensor_dict = {}\n",
        "            for key in [\n",
        "                'num_detections', 'detection_boxes', 'detection_scores',\n",
        "                'detection_classes', 'detection_masks'\n",
        "            ]:\n",
        "                tensor_name = key + ':0'\n",
        "                if tensor_name in all_tensor_names:\n",
        "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
        "                        tensor_name)\n",
        "            if 'detection_masks' in tensor_dict:\n",
        "                # The following processing is only for single image\n",
        "                detection_boxes = tf.squeeze(\n",
        "                    tensor_dict['detection_boxes'], [0])\n",
        "                detection_masks = tf.squeeze(\n",
        "                    tensor_dict['detection_masks'], [0])\n",
        "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
        "                real_num_detection = tf.cast(\n",
        "                    tensor_dict['num_detections'][0], tf.int32)\n",
        "                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n",
        "                                           real_num_detection, -1])\n",
        "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n",
        "                                           real_num_detection, -1, -1])\n",
        "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
        "                detection_masks_reframed = tf.cast(\n",
        "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
        "                # Follow the convention by adding back the batch dimension\n",
        "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
        "                    detection_masks_reframed, 0)\n",
        "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "            # Run inference\n",
        "            output_dict = sess.run(tensor_dict,\n",
        "                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
        "\n",
        "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "            output_dict['num_detections'] = int(\n",
        "                output_dict['num_detections'][0])\n",
        "            output_dict['detection_classes'] = output_dict[\n",
        "                'detection_classes'][0].astype(np.uint8)\n",
        "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
        "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
        "            if 'detection_masks' in output_dict:\n",
        "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
        "    return output_dict\n",
        "\n",
        "\n",
        "for image_path in TEST_IMAGE_PATHS:\n",
        "    image = Image.open(image_path)\n",
        "    print(image_path)\n",
        "    # the array based representation of the image will be used later in order to prepare the\n",
        "    # result image with boxes and labels on it.\n",
        "    image_np = load_image_into_numpy_array(image)\n",
        "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
        "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
        "    # Actual detection.\n",
        "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
        "    # Visualization of the results of a detection.\n",
        "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "        image_np,\n",
        "        output_dict['detection_boxes'],\n",
        "        output_dict['detection_classes'],\n",
        "        output_dict['detection_scores'],\n",
        "        category_index,\n",
        "        instance_masks=output_dict.get('detection_masks'),\n",
        "        use_normalized_coordinates=True,\n",
        "        line_thickness=8)\n",
        "    plt.figure(figsize=IMAGE_SIZE)\n",
        "    plt.imshow(image_np)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/object_detection\n",
            "/content/tensorflow-object-detection-faster-rcnn/data/test/language.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PlfZAR1OCK2"
      },
      "source": [
        "# Convert TF model to OpenVINO 20.01 Intermediate Representation (IR)\n",
        " This can be used to run inference on OpenVINO.\n",
        "# In order to run the model on DepthAI modules, we then compile the IR obtained above to a .blob (via a server we set up just for that) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qucQJwsWhL3"
      },
      "source": [
        "## First, we install Open Vino 20.01\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BxCxkQ6NkcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab9d9c9-3f92-4f51-94fa-ed2fb090d004"
      },
      "source": [
        "%cd ../.."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdxrnAQHYVc3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5py5tfxS6VaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2a1c465-ab84-4f62-90e9-702be7849b79"
      },
      "source": [
        "%%time\n",
        "# %%capture\n",
        "## install tools. Open Vino takes some time to download: 10-15 min sometimes.\n",
        "!sudo apt-get install -y pciutils cpio\n",
        "!sudo apt autoremove\n",
        "## downnload installation files\n",
        "!wget http://registrationcenter-download.intel.com/akdlm/irc_nas/16345/l_openvino_toolkit_p_2020.1.023.tgz\n",
        "path = \"l_openvino_toolkit_p_2020.1.023.tgz\"\n",
        "# path = \"/content/software/Intel OpenVINO 2019 R3.1/l_openvino_toolkit_p_2019.3.376.tgz\"\n",
        "## install openvino\n",
        "!tar xf \"{path}\"\n",
        "# !tar xf \"{path}\" && \\\n",
        "#     cd l_openvino_toolkit_p* && \\\n",
        "#     ./install_openvino_dependencies.sh && \\\n",
        "#     sed -i 's/decline/accept/g' silent.cfg && \\\n",
        "#     ./install.sh\n",
        "# ## install dependencies\n",
        "# !/opt/intel/openvino/install_dependencies/install_openvino_dependencies.sh\n",
        "# ## install prerequisites\n",
        "# !/opt/intel/openvino/deployment_tools/model_optimizer/install_prerequisites/install_prerequisites.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpci3\n",
            "Suggested packages:\n",
            "  libarchive1\n",
            "The following NEW packages will be installed:\n",
            "  cpio libpci3 pciutils\n",
            "0 upgraded, 3 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 368 kB of archives.\n",
            "After this operation, 1,786 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 cpio amd64 2.12+dfsg-6ubuntu0.18.04.1 [86.2 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpci3 amd64 1:3.5.2-1ubuntu1.1 [24.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 pciutils amd64 1:3.5.2-1ubuntu1.1 [257 kB]\n",
            "Fetched 368 kB in 0s (3,936 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package cpio.\n",
            "(Reading database ... 160837 files and directories currently installed.)\n",
            "Preparing to unpack .../cpio_2.12+dfsg-6ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking cpio (2.12+dfsg-6ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package libpci3:amd64.\n",
            "Preparing to unpack .../libpci3_1%3a3.5.2-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libpci3:amd64 (1:3.5.2-1ubuntu1.1) ...\n",
            "Selecting previously unselected package pciutils.\n",
            "Preparing to unpack .../pciutils_1%3a3.5.2-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking pciutils (1:3.5.2-1ubuntu1.1) ...\n",
            "Setting up cpio (2.12+dfsg-6ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /bin/mt-gnu to provide /bin/mt (mt) in auto mode\n",
            "Setting up libpci3:amd64 (1:3.5.2-1ubuntu1.1) ...\n",
            "Setting up pciutils (1:3.5.2-1ubuntu1.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n",
            "--2021-07-29 08:53:06--  http://registrationcenter-download.intel.com/akdlm/irc_nas/16345/l_openvino_toolkit_p_2020.1.023.tgz\n",
            "Resolving registrationcenter-download.intel.com (registrationcenter-download.intel.com)... 2.18.245.56, 2.18.245.41\n",
            "Connecting to registrationcenter-download.intel.com (registrationcenter-download.intel.com)|2.18.245.56|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 508213676 (485M) [application/octet-stream]\n",
            "Saving to: ‘l_openvino_toolkit_p_2020.1.023.tgz’\n",
            "\n",
            "l_openvino_toolkit_ 100%[===================>] 484.67M   192MB/s    in 2.5s    \n",
            "\n",
            "2021-07-29 08:53:09 (192 MB/s) - ‘l_openvino_toolkit_p_2020.1.023.tgz’ saved [508213676/508213676]\n",
            "\n",
            "CPU times: user 232 ms, sys: 87.4 ms, total: 319 ms\n",
            "Wall time: 21.1 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNFUanul7s1g"
      },
      "source": [
        "%%capture\n",
        "%cd l_openvino_toolkit_p_2020.1.023/\n",
        "!./install_openvino_dependencies.sh && \\\n",
        "    sed -i 's/decline/accept/g' silent.cfg && \\\n",
        "    ./install.sh --silent silent.cfg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2enKxK_qakX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e83a52f-6aa1-4319-e358-4a5b75549d82"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EULA.txt\tinstall_openvino_dependencies.sh  pset\t\t  rpm\n",
            "install_GUI.sh\tinstall.sh\t\t\t  PUBLIC_KEY.PUB  silent.cfg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-jbjiw-7nAv"
      },
      "source": [
        "[Optional] Open Vino install check, generally not needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NJp28Oj7Ts9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa114315-7ad0-4a18-bd22-f6ce7eaa93a6"
      },
      "source": [
        "!source /opt/intel/openvino/bin/setupvars.sh && \\\n",
        "    /opt/intel/openvino/deployment_tools/demo/demo_squeezenet_download_convert_run.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[setupvars.sh] OpenVINO environment initialized\n",
            "target_precision = FP16\n",
            "[setupvars.sh] OpenVINO environment initialized\n",
            "\n",
            "\n",
            "###################################################\n",
            "\n",
            "\n",
            "\n",
            "Downloading the Caffe model and the prototxt\n",
            "Installing dependencies\n",
            "Hit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:4 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:6 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:8 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:9 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "90 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Run sudo -E apt -y install build-essential python3-pip virtualenv cmake libcairo2-dev libpango1.0-dev libglib2.0-dev libgtk2.0-dev libswscale-dev libavcodec-dev libavformat-dev libgstreamer1.0-0 gstreamer1.0-plugins-base\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.4ubuntu1).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n",
            "gstreamer1.0-plugins-base is already the newest version (1.14.5-0ubuntu1~18.04.3).\n",
            "libglib2.0-dev is already the newest version (2.56.4-0ubuntu0.18.04.8).\n",
            "libglib2.0-dev set to manually installed.\n",
            "libgstreamer1.0-0 is already the newest version (1.14.5-0ubuntu1~18.04.2).\n",
            "libavcodec-dev is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "libavcodec-dev set to manually installed.\n",
            "libavformat-dev is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "libavformat-dev set to manually installed.\n",
            "libswscale-dev is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "libswscale-dev set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  autoconf automake autopoint autotools-dev debhelper dh-autoreconf\n",
            "  dh-strip-nondeterminism file gettext gettext-base gir1.2-atk-1.0\n",
            "  gir1.2-freedesktop gir1.2-gdkpixbuf-2.0 gir1.2-gtk-2.0 gir1.2-pango-1.0\n",
            "  intltool-debian libarchive-cpio-perl libarchive-zip-perl libatk1.0-dev\n",
            "  libcairo-script-interpreter2 libfile-stripnondeterminism-perl libgail-common\n",
            "  libgail18 libgdk-pixbuf2.0-dev libgtk2.0-0 libgtk2.0-bin libgtk2.0-common\n",
            "  libmagic-mgc libmagic1 libmail-sendmail-perl libpangoxft-1.0-0\n",
            "  libpixman-1-dev libsigsegv2 libsys-hostname-long-perl libtimedate-perl\n",
            "  libtool libxcb-shm0-dev libxcomposite-dev libxcursor-dev libxinerama-dev\n",
            "  libxml2-utils libxrandr-dev m4 po-debconf python-pip-whl python3-asn1crypto\n",
            "  python3-cffi-backend python3-crypto python3-cryptography python3-idna\n",
            "  python3-keyring python3-keyrings.alt python3-pkg-resources\n",
            "  python3-secretstorage python3-setuptools python3-six python3-virtualenv\n",
            "  python3-wheel python3-xdg x11proto-composite-dev x11proto-randr-dev\n",
            "  x11proto-xinerama-dev\n",
            "Suggested packages:\n",
            "  autoconf-archive gnu-standards autoconf-doc dh-make dwz gettext-doc\n",
            "  libasprintf-dev libgettextpo-dev libcairo2-doc gvfs libgtk2.0-doc\n",
            "  imagemagick libpango1.0-doc libtool-doc gcj-jdk m4-doc libmail-box-perl\n",
            "  python-crypto-doc python-cryptography-doc python3-cryptography-vectors\n",
            "  gnome-keyring libkf5wallet-bin gir1.2-gnomekeyring-1.0\n",
            "  python-secretstorage-doc python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  autoconf automake autopoint autotools-dev debhelper dh-autoreconf\n",
            "  dh-strip-nondeterminism file gettext gettext-base gir1.2-atk-1.0\n",
            "  gir1.2-freedesktop gir1.2-gdkpixbuf-2.0 gir1.2-gtk-2.0 gir1.2-pango-1.0\n",
            "  intltool-debian libarchive-cpio-perl libarchive-zip-perl libatk1.0-dev\n",
            "  libcairo-script-interpreter2 libcairo2-dev libfile-stripnondeterminism-perl\n",
            "  libgail-common libgail18 libgdk-pixbuf2.0-dev libgtk2.0-0 libgtk2.0-bin\n",
            "  libgtk2.0-common libgtk2.0-dev libmagic-mgc libmagic1 libmail-sendmail-perl\n",
            "  libpango1.0-dev libpangoxft-1.0-0 libpixman-1-dev libsigsegv2\n",
            "  libsys-hostname-long-perl libtimedate-perl libtool libxcb-shm0-dev\n",
            "  libxcomposite-dev libxcursor-dev libxinerama-dev libxml2-utils libxrandr-dev\n",
            "  m4 po-debconf python-pip-whl python3-asn1crypto python3-cffi-backend\n",
            "  python3-crypto python3-cryptography python3-idna python3-keyring\n",
            "  python3-keyrings.alt python3-pip python3-pkg-resources python3-secretstorage\n",
            "  python3-setuptools python3-six python3-virtualenv python3-wheel python3-xdg\n",
            "  virtualenv x11proto-composite-dev x11proto-randr-dev x11proto-xinerama-dev\n",
            "0 upgraded, 67 newly installed, 0 to remove and 90 not upgraded.\n",
            "Need to get 14.1 MB of archives.\n",
            "After this operation, 66.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gettext-base amd64 0.19.8.1-6ubuntu0.3 [113 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsigsegv2 amd64 2.12-1 [14.7 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 m4 amd64 1.4.18-1 [197 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/main amd64 autoconf all 2.69-11 [322 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 autotools-dev all 20180224.1 [39.6 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic/main amd64 automake all 1:1.15.1-3ubuntu2 [509 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 autopoint all 0.19.8.1-6ubuntu0.3 [426 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtool all 2.4.6-2 [194 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic/main amd64 dh-autoreconf all 17 [15.8 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libarchive-zip-perl all 1.60-1ubuntu0.1 [84.6 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic/main amd64 libfile-stripnondeterminism-perl all 0.040-1.1~build1 [13.8 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic/main amd64 libtimedate-perl all 2.3000-2 [37.5 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic/main amd64 dh-strip-nondeterminism all 0.040-1.1~build1 [5,208 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gettext amd64 0.19.8.1-6ubuntu0.3 [1,293 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic/main amd64 intltool-debian all 0.35.0+20060710.4 [24.9 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic/main amd64 po-debconf all 1.0.20 [232 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 debhelper all 11.1.6ubuntu2 [902 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-atk-1.0 amd64 2.28.1-1 [17.8 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-freedesktop amd64 1.56.1-1 [9,080 B]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-gdkpixbuf-2.0 amd64 2.36.11-2 [7,748 B]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-common all 2.24.32-1ubuntu1 [125 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpangoxft-1.0-0 amd64 1.40.14-1ubuntu0.1 [15.0 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gir1.2-pango-1.0 amd64 1.40.14-1ubuntu0.1 [21.6 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-0 amd64 2.24.32-1ubuntu1 [1,769 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic/main amd64 gir1.2-gtk-2.0 amd64 2.24.32-1ubuntu1 [172 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic/main amd64 libarchive-cpio-perl all 0.10-1 [9,644 B]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic/main amd64 libatk1.0-dev amd64 2.28.1-1 [79.9 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcairo-script-interpreter2 amd64 1.15.10-2ubuntu0.1 [53.5 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic/main amd64 libpixman-1-dev amd64 0.34.0-2 [244 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxcb-shm0-dev amd64 1.13-2~ubuntu18.04 [6,684 B]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcairo2-dev amd64 1.15.10-2ubuntu0.1 [626 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail18 amd64 2.24.32-1ubuntu1 [14.2 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgail-common amd64 2.24.32-1ubuntu1 [112 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgdk-pixbuf2.0-dev amd64 2.36.11-2 [46.8 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-bin amd64 2.24.32-1ubuntu1 [7,536 B]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpango1.0-dev amd64 1.40.14-1ubuntu0.1 [288 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-xinerama-dev all 2018.4-4 [2,628 B]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxinerama-dev amd64 2:1.1.3-1 [8,404 B]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-randr-dev all 2018.4-4 [2,620 B]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxrandr-dev amd64 2:1.5.1-1 [24.0 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcursor-dev amd64 1:1.1.15-1 [26.5 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11proto-composite-dev all 1:2018.4-4 [2,620 B]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxcomposite-dev amd64 1:0.4.4-2 [9,136 B]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libxml2-utils amd64 2.9.4+dfsg1-6.1ubuntu1.4 [35.8 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu bionic/main amd64 libgtk2.0-dev amd64 2.24.32-1ubuntu1 [2,652 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu bionic/main amd64 libsys-hostname-long-perl all 1.5-1 [11.7 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu bionic/main amd64 libmail-sendmail-perl all 0.80-1 [22.6 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python-pip-whl all 9.0.1-2.3~ubuntu1.18.04.5 [1,653 kB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-asn1crypto all 0.24.0-1 [72.8 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-cffi-backend amd64 1.11.5-1 [64.6 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-crypto amd64 2.6.1-8ubuntu2 [244 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-idna all 2.6-1 [32.5 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-six all 1.11.0-2 [11.4 kB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-cryptography amd64 2.1.4-1ubuntu1.4 [220 kB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-secretstorage all 2.3.1-2 [12.1 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyring all 10.6.0-1 [26.7 kB]\n",
            "Get:60 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-keyrings.alt all 3.0-1 [16.6 kB]\n",
            "Get:61 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3-pip all 9.0.1-2.3~ubuntu1.18.04.5 [114 kB]\n",
            "Get:62 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-pkg-resources all 39.0.1-2 [98.8 kB]\n",
            "Get:63 http://archive.ubuntu.com/ubuntu bionic/main amd64 python3-setuptools all 39.0.1-2 [248 kB]\n",
            "Get:64 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-virtualenv all 15.1.0+ds-1.1 [43.4 kB]\n",
            "Get:65 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python3-wheel all 0.30.0-0.2 [36.5 kB]\n",
            "Get:66 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 python3-xdg all 0.25-4ubuntu1.1 [31.3 kB]\n",
            "Get:67 http://archive.ubuntu.com/ubuntu bionic/universe amd64 virtualenv all 15.1.0+ds-1.1 [4,476 B]\n",
            "Fetched 14.1 MB in 0s (36.9 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 161850 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../01-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../02-file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package gettext-base.\n",
            "Preparing to unpack .../03-gettext-base_0.19.8.1-6ubuntu0.3_amd64.deb ...\n",
            "Unpacking gettext-base (0.19.8.1-6ubuntu0.3) ...\n",
            "Selecting previously unselected package libsigsegv2:amd64.\n",
            "Preparing to unpack .../04-libsigsegv2_2.12-1_amd64.deb ...\n",
            "Unpacking libsigsegv2:amd64 (2.12-1) ...\n",
            "Selecting previously unselected package m4.\n",
            "Preparing to unpack .../05-m4_1.4.18-1_amd64.deb ...\n",
            "Unpacking m4 (1.4.18-1) ...\n",
            "Selecting previously unselected package autoconf.\n",
            "Preparing to unpack .../06-autoconf_2.69-11_all.deb ...\n",
            "Unpacking autoconf (2.69-11) ...\n",
            "Selecting previously unselected package autotools-dev.\n",
            "Preparing to unpack .../07-autotools-dev_20180224.1_all.deb ...\n",
            "Unpacking autotools-dev (20180224.1) ...\n",
            "Selecting previously unselected package automake.\n",
            "Preparing to unpack .../08-automake_1%3a1.15.1-3ubuntu2_all.deb ...\n",
            "Unpacking automake (1:1.15.1-3ubuntu2) ...\n",
            "Selecting previously unselected package autopoint.\n",
            "Preparing to unpack .../09-autopoint_0.19.8.1-6ubuntu0.3_all.deb ...\n",
            "Unpacking autopoint (0.19.8.1-6ubuntu0.3) ...\n",
            "Selecting previously unselected package libtool.\n",
            "Preparing to unpack .../10-libtool_2.4.6-2_all.deb ...\n",
            "Unpacking libtool (2.4.6-2) ...\n",
            "Selecting previously unselected package dh-autoreconf.\n",
            "Preparing to unpack .../11-dh-autoreconf_17_all.deb ...\n",
            "Unpacking dh-autoreconf (17) ...\n",
            "Selecting previously unselected package libarchive-zip-perl.\n",
            "Preparing to unpack .../12-libarchive-zip-perl_1.60-1ubuntu0.1_all.deb ...\n",
            "Unpacking libarchive-zip-perl (1.60-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libfile-stripnondeterminism-perl.\n",
            "Preparing to unpack .../13-libfile-stripnondeterminism-perl_0.040-1.1~build1_all.deb ...\n",
            "Unpacking libfile-stripnondeterminism-perl (0.040-1.1~build1) ...\n",
            "Selecting previously unselected package libtimedate-perl.\n",
            "Preparing to unpack .../14-libtimedate-perl_2.3000-2_all.deb ...\n",
            "Unpacking libtimedate-perl (2.3000-2) ...\n",
            "Selecting previously unselected package dh-strip-nondeterminism.\n",
            "Preparing to unpack .../15-dh-strip-nondeterminism_0.040-1.1~build1_all.deb ...\n",
            "Unpacking dh-strip-nondeterminism (0.040-1.1~build1) ...\n",
            "Selecting previously unselected package gettext.\n",
            "Preparing to unpack .../16-gettext_0.19.8.1-6ubuntu0.3_amd64.deb ...\n",
            "Unpacking gettext (0.19.8.1-6ubuntu0.3) ...\n",
            "Selecting previously unselected package intltool-debian.\n",
            "Preparing to unpack .../17-intltool-debian_0.35.0+20060710.4_all.deb ...\n",
            "Unpacking intltool-debian (0.35.0+20060710.4) ...\n",
            "Selecting previously unselected package po-debconf.\n",
            "Preparing to unpack .../18-po-debconf_1.0.20_all.deb ...\n",
            "Unpacking po-debconf (1.0.20) ...\n",
            "Selecting previously unselected package debhelper.\n",
            "Preparing to unpack .../19-debhelper_11.1.6ubuntu2_all.deb ...\n",
            "Unpacking debhelper (11.1.6ubuntu2) ...\n",
            "Selecting previously unselected package gir1.2-atk-1.0:amd64.\n",
            "Preparing to unpack .../20-gir1.2-atk-1.0_2.28.1-1_amd64.deb ...\n",
            "Unpacking gir1.2-atk-1.0:amd64 (2.28.1-1) ...\n",
            "Selecting previously unselected package gir1.2-freedesktop:amd64.\n",
            "Preparing to unpack .../21-gir1.2-freedesktop_1.56.1-1_amd64.deb ...\n",
            "Unpacking gir1.2-freedesktop:amd64 (1.56.1-1) ...\n",
            "Selecting previously unselected package gir1.2-gdkpixbuf-2.0:amd64.\n",
            "Preparing to unpack .../22-gir1.2-gdkpixbuf-2.0_2.36.11-2_amd64.deb ...\n",
            "Unpacking gir1.2-gdkpixbuf-2.0:amd64 (2.36.11-2) ...\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "Preparing to unpack .../23-libgtk2.0-common_2.24.32-1ubuntu1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libpangoxft-1.0-0:amd64.\n",
            "Preparing to unpack .../24-libpangoxft-1.0-0_1.40.14-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libpangoxft-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Selecting previously unselected package gir1.2-pango-1.0:amd64.\n",
            "Preparing to unpack .../25-gir1.2-pango-1.0_1.40.14-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking gir1.2-pango-1.0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../26-libgtk2.0-0_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package gir1.2-gtk-2.0.\n",
            "Preparing to unpack .../27-gir1.2-gtk-2.0_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking gir1.2-gtk-2.0 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libarchive-cpio-perl.\n",
            "Preparing to unpack .../28-libarchive-cpio-perl_0.10-1_all.deb ...\n",
            "Unpacking libarchive-cpio-perl (0.10-1) ...\n",
            "Selecting previously unselected package libatk1.0-dev:amd64.\n",
            "Preparing to unpack .../29-libatk1.0-dev_2.28.1-1_amd64.deb ...\n",
            "Unpacking libatk1.0-dev:amd64 (2.28.1-1) ...\n",
            "Selecting previously unselected package libcairo-script-interpreter2:amd64.\n",
            "Preparing to unpack .../30-libcairo-script-interpreter2_1.15.10-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libcairo-script-interpreter2:amd64 (1.15.10-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libpixman-1-dev:amd64.\n",
            "Preparing to unpack .../31-libpixman-1-dev_0.34.0-2_amd64.deb ...\n",
            "Unpacking libpixman-1-dev:amd64 (0.34.0-2) ...\n",
            "Selecting previously unselected package libxcb-shm0-dev:amd64.\n",
            "Preparing to unpack .../32-libxcb-shm0-dev_1.13-2~ubuntu18.04_amd64.deb ...\n",
            "Unpacking libxcb-shm0-dev:amd64 (1.13-2~ubuntu18.04) ...\n",
            "Selecting previously unselected package libcairo2-dev:amd64.\n",
            "Preparing to unpack .../33-libcairo2-dev_1.15.10-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libcairo2-dev:amd64 (1.15.10-2ubuntu0.1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../34-libgail18_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../35-libgail-common_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libgdk-pixbuf2.0-dev.\n",
            "Preparing to unpack .../36-libgdk-pixbuf2.0-dev_2.36.11-2_amd64.deb ...\n",
            "Unpacking libgdk-pixbuf2.0-dev (2.36.11-2) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../37-libgtk2.0-bin_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libpango1.0-dev.\n",
            "Preparing to unpack .../38-libpango1.0-dev_1.40.14-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libpango1.0-dev (1.40.14-1ubuntu0.1) ...\n",
            "Selecting previously unselected package x11proto-xinerama-dev.\n",
            "Preparing to unpack .../39-x11proto-xinerama-dev_2018.4-4_all.deb ...\n",
            "Unpacking x11proto-xinerama-dev (2018.4-4) ...\n",
            "Selecting previously unselected package libxinerama-dev:amd64.\n",
            "Preparing to unpack .../40-libxinerama-dev_2%3a1.1.3-1_amd64.deb ...\n",
            "Unpacking libxinerama-dev:amd64 (2:1.1.3-1) ...\n",
            "Selecting previously unselected package x11proto-randr-dev.\n",
            "Preparing to unpack .../41-x11proto-randr-dev_2018.4-4_all.deb ...\n",
            "Unpacking x11proto-randr-dev (2018.4-4) ...\n",
            "Selecting previously unselected package libxrandr-dev:amd64.\n",
            "Preparing to unpack .../42-libxrandr-dev_2%3a1.5.1-1_amd64.deb ...\n",
            "Unpacking libxrandr-dev:amd64 (2:1.5.1-1) ...\n",
            "Selecting previously unselected package libxcursor-dev:amd64.\n",
            "Preparing to unpack .../43-libxcursor-dev_1%3a1.1.15-1_amd64.deb ...\n",
            "Unpacking libxcursor-dev:amd64 (1:1.1.15-1) ...\n",
            "Selecting previously unselected package x11proto-composite-dev.\n",
            "Preparing to unpack .../44-x11proto-composite-dev_1%3a2018.4-4_all.deb ...\n",
            "Unpacking x11proto-composite-dev (1:2018.4-4) ...\n",
            "Selecting previously unselected package libxcomposite-dev:amd64.\n",
            "Preparing to unpack .../45-libxcomposite-dev_1%3a0.4.4-2_amd64.deb ...\n",
            "Unpacking libxcomposite-dev:amd64 (1:0.4.4-2) ...\n",
            "Selecting previously unselected package libxml2-utils.\n",
            "Preparing to unpack .../46-libxml2-utils_2.9.4+dfsg1-6.1ubuntu1.4_amd64.deb ...\n",
            "Unpacking libxml2-utils (2.9.4+dfsg1-6.1ubuntu1.4) ...\n",
            "Selecting previously unselected package libgtk2.0-dev.\n",
            "Preparing to unpack .../47-libgtk2.0-dev_2.24.32-1ubuntu1_amd64.deb ...\n",
            "Unpacking libgtk2.0-dev (2.24.32-1ubuntu1) ...\n",
            "Selecting previously unselected package libsys-hostname-long-perl.\n",
            "Preparing to unpack .../48-libsys-hostname-long-perl_1.5-1_all.deb ...\n",
            "Unpacking libsys-hostname-long-perl (1.5-1) ...\n",
            "Selecting previously unselected package libmail-sendmail-perl.\n",
            "Preparing to unpack .../49-libmail-sendmail-perl_0.80-1_all.deb ...\n",
            "Unpacking libmail-sendmail-perl (0.80-1) ...\n",
            "Selecting previously unselected package python-pip-whl.\n",
            "Preparing to unpack .../50-python-pip-whl_9.0.1-2.3~ubuntu1.18.04.5_all.deb ...\n",
            "Unpacking python-pip-whl (9.0.1-2.3~ubuntu1.18.04.5) ...\n",
            "Selecting previously unselected package python3-asn1crypto.\n",
            "Preparing to unpack .../51-python3-asn1crypto_0.24.0-1_all.deb ...\n",
            "Unpacking python3-asn1crypto (0.24.0-1) ...\n",
            "Selecting previously unselected package python3-cffi-backend.\n",
            "Preparing to unpack .../52-python3-cffi-backend_1.11.5-1_amd64.deb ...\n",
            "Unpacking python3-cffi-backend (1.11.5-1) ...\n",
            "Selecting previously unselected package python3-crypto.\n",
            "Preparing to unpack .../53-python3-crypto_2.6.1-8ubuntu2_amd64.deb ...\n",
            "Unpacking python3-crypto (2.6.1-8ubuntu2) ...\n",
            "Selecting previously unselected package python3-idna.\n",
            "Preparing to unpack .../54-python3-idna_2.6-1_all.deb ...\n",
            "Unpacking python3-idna (2.6-1) ...\n",
            "Selecting previously unselected package python3-six.\n",
            "Preparing to unpack .../55-python3-six_1.11.0-2_all.deb ...\n",
            "Unpacking python3-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python3-cryptography.\n",
            "Preparing to unpack .../56-python3-cryptography_2.1.4-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking python3-cryptography (2.1.4-1ubuntu1.4) ...\n",
            "Selecting previously unselected package python3-secretstorage.\n",
            "Preparing to unpack .../57-python3-secretstorage_2.3.1-2_all.deb ...\n",
            "Unpacking python3-secretstorage (2.3.1-2) ...\n",
            "Selecting previously unselected package python3-keyring.\n",
            "Preparing to unpack .../58-python3-keyring_10.6.0-1_all.deb ...\n",
            "Unpacking python3-keyring (10.6.0-1) ...\n",
            "Selecting previously unselected package python3-keyrings.alt.\n",
            "Preparing to unpack .../59-python3-keyrings.alt_3.0-1_all.deb ...\n",
            "Unpacking python3-keyrings.alt (3.0-1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../60-python3-pip_9.0.1-2.3~ubuntu1.18.04.5_all.deb ...\n",
            "Unpacking python3-pip (9.0.1-2.3~ubuntu1.18.04.5) ...\n",
            "Selecting previously unselected package python3-pkg-resources.\n",
            "Preparing to unpack .../61-python3-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python3-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../62-python3-setuptools_39.0.1-2_all.deb ...\n",
            "Unpacking python3-setuptools (39.0.1-2) ...\n",
            "Selecting previously unselected package python3-virtualenv.\n",
            "Preparing to unpack .../63-python3-virtualenv_15.1.0+ds-1.1_all.deb ...\n",
            "Unpacking python3-virtualenv (15.1.0+ds-1.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../64-python3-wheel_0.30.0-0.2_all.deb ...\n",
            "Unpacking python3-wheel (0.30.0-0.2) ...\n",
            "Selecting previously unselected package python3-xdg.\n",
            "Preparing to unpack .../65-python3-xdg_0.25-4ubuntu1.1_all.deb ...\n",
            "Unpacking python3-xdg (0.25-4ubuntu1.1) ...\n",
            "Selecting previously unselected package virtualenv.\n",
            "Preparing to unpack .../66-virtualenv_15.1.0+ds-1.1_all.deb ...\n",
            "Unpacking virtualenv (15.1.0+ds-1.1) ...\n",
            "Setting up python-pip-whl (9.0.1-2.3~ubuntu1.18.04.5) ...\n",
            "Setting up libxcursor-dev:amd64 (1:1.1.15-1) ...\n",
            "Setting up gir1.2-atk-1.0:amd64 (2.28.1-1) ...\n",
            "Setting up libgtk2.0-common (2.24.32-1ubuntu1) ...\n",
            "Setting up libarchive-zip-perl (1.60-1ubuntu0.1) ...\n",
            "Setting up python3-cffi-backend (1.11.5-1) ...\n",
            "Setting up libtimedate-perl (2.3000-2) ...\n",
            "Setting up libcairo-script-interpreter2:amd64 (1.15.10-2ubuntu0.1) ...\n",
            "Setting up libsigsegv2:amd64 (2.12-1) ...\n",
            "Setting up python3-crypto (2.6.1-8ubuntu2) ...\n",
            "Setting up gir1.2-freedesktop:amd64 (1.56.1-1) ...\n",
            "Setting up python3-idna (2.6-1) ...\n",
            "Setting up python3-xdg (0.25-4ubuntu1.1) ...\n",
            "Setting up libxcb-shm0-dev:amd64 (1.13-2~ubuntu18.04) ...\n",
            "Setting up python3-six (1.11.0-2) ...\n",
            "Setting up libpangoxft-1.0-0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Setting up libxml2-utils (2.9.4+dfsg1-6.1ubuntu1.4) ...\n",
            "Setting up libarchive-cpio-perl (0.10-1) ...\n",
            "Setting up gir1.2-gdkpixbuf-2.0:amd64 (2.36.11-2) ...\n",
            "Setting up libatk1.0-dev:amd64 (2.28.1-1) ...\n",
            "Setting up gettext-base (0.19.8.1-6ubuntu0.3) ...\n",
            "Setting up python3-wheel (0.30.0-0.2) ...\n",
            "Setting up m4 (1.4.18-1) ...\n",
            "Setting up python3-pkg-resources (39.0.1-2) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up gir1.2-pango-1.0:amd64 (1.40.14-1ubuntu0.1) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up python3-asn1crypto (0.24.0-1) ...\n",
            "Setting up libsys-hostname-long-perl (1.5-1) ...\n",
            "Setting up libgdk-pixbuf2.0-dev (2.36.11-2) ...\n",
            "Setting up libmail-sendmail-perl (0.80-1) ...\n",
            "Setting up x11proto-xinerama-dev (2018.4-4) ...\n",
            "Setting up autotools-dev (20180224.1) ...\n",
            "Setting up libpixman-1-dev:amd64 (0.34.0-2) ...\n",
            "Setting up x11proto-randr-dev (2018.4-4) ...\n",
            "Setting up libxinerama-dev:amd64 (2:1.1.3-1) ...\n",
            "Setting up python3-virtualenv (15.1.0+ds-1.1) ...\n",
            "Setting up python3-pip (9.0.1-2.3~ubuntu1.18.04.5) ...\n",
            "Setting up python3-setuptools (39.0.1-2) ...\n",
            "Setting up virtualenv (15.1.0+ds-1.1) ...\n",
            "Setting up x11proto-composite-dev (1:2018.4-4) ...\n",
            "Setting up autopoint (0.19.8.1-6ubuntu0.3) ...\n",
            "Setting up libfile-stripnondeterminism-perl (0.040-1.1~build1) ...\n",
            "Setting up python3-cryptography (2.1.4-1ubuntu1.4) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgail18:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up libxrandr-dev:amd64 (2:1.5.1-1) ...\n",
            "Setting up libcairo2-dev:amd64 (1.15.10-2ubuntu0.1) ...\n",
            "Setting up gettext (0.19.8.1-6ubuntu0.3) ...\n",
            "Setting up libxcomposite-dev:amd64 (1:0.4.4-2) ...\n",
            "Setting up python3-keyrings.alt (3.0-1) ...\n",
            "Setting up libgail-common:amd64 (2.24.32-1ubuntu1) ...\n",
            "Setting up autoconf (2.69-11) ...\n",
            "Setting up file (1:5.32-2ubuntu0.4) ...\n",
            "Setting up intltool-debian (0.35.0+20060710.4) ...\n",
            "Setting up automake (1:1.15.1-3ubuntu2) ...\n",
            "update-alternatives: using /usr/bin/automake-1.15 to provide /usr/bin/automake (automake) in auto mode\n",
            "Setting up libpango1.0-dev (1.40.14-1ubuntu0.1) ...\n",
            "Setting up python3-secretstorage (2.3.1-2) ...\n",
            "Setting up gir1.2-gtk-2.0 (2.24.32-1ubuntu1) ...\n",
            "Setting up libgtk2.0-bin (2.24.32-1ubuntu1) ...\n",
            "Setting up libtool (2.4.6-2) ...\n",
            "Setting up po-debconf (1.0.20) ...\n",
            "Setting up python3-keyring (10.6.0-1) ...\n",
            "Setting up libgtk2.0-dev (2.24.32-1ubuntu1) ...\n",
            "Setting up dh-autoreconf (17) ...\n",
            "Setting up debhelper (11.1.6ubuntu2) ...\n",
            "Setting up dh-strip-nondeterminism (0.040-1.1~build1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libpng-dev is already the newest version (1.6.34-1ubuntu0.18.04.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 90 not upgraded.\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from -r /opt/intel/openvino/deployment_tools/demo/../open_model_zoo/tools/downloader/requirements.in (line 1)) (3.13)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r /opt/intel/openvino/deployment_tools/demo/../open_model_zoo/tools/downloader/requirements.in (line 2)) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->-r /opt/intel/openvino/deployment_tools/demo/../open_model_zoo/tools/downloader/requirements.in (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r /opt/intel/openvino/deployment_tools/demo/../open_model_zoo/tools/downloader/requirements.in (line 2)) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r /opt/intel/openvino/deployment_tools/demo/../open_model_zoo/tools/downloader/requirements.in (line 2)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r /opt/intel/openvino/deployment_tools/demo/../open_model_zoo/tools/downloader/requirements.in (line 2)) (2.10)\n",
            "Run python3 /opt/intel/openvino_2020.1.023/deployment_tools/open_model_zoo/tools/downloader/downloader.py --name squeezenet1.1 --output_dir /root/openvino_models/models --cache_dir /root/openvino_models/cache\n",
            "\n",
            "################|| Downloading models ||################\n",
            "\n",
            "========== Downloading /root/openvino_models/models/public/squeezenet1.1/squeezenet1.1.prototxt\n",
            "... 100%, 9 KB, 47127 KB/s, 0 seconds passed\n",
            "\n",
            "========== Downloading /root/openvino_models/models/public/squeezenet1.1/squeezenet1.1.caffemodel\n",
            "... 100%, 4834 KB, 30267 KB/s, 0 seconds passed\n",
            "\n",
            "################|| Post-processing ||################\n",
            "\n",
            "========== Replacing text in /root/openvino_models/models/public/squeezenet1.1/squeezenet1.1.prototxt\n",
            "\n",
            "\n",
            "###################################################\n",
            "\n",
            "Install Model Optimizer dependencies\n",
            "\n",
            "Hit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:4 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:9 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "90 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3-pip is already the newest version (9.0.1-2.3~ubuntu1.18.04.5).\n",
            "The following additional packages will be installed:\n",
            "  gcc-6-base python3.6-venv\n",
            "The following NEW packages will be installed:\n",
            "  gcc-6-base libgfortran3 python3-venv python3.6-venv\n",
            "0 upgraded, 4 newly installed, 0 to remove and 90 not upgraded.\n",
            "Need to get 294 kB of archives.\n",
            "After this operation, 1,438 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 gcc-6-base amd64 6.5.0-2ubuntu1~18.04 [16.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libgfortran3 amd64 6.5.0-2ubuntu1~18.04 [270 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3.6-venv amd64 3.6.9-1~18.04ubuntu1.4 [6,188 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 python3-venv amd64 3.6.7-1~18.04 [1,208 B]\n",
            "Fetched 294 kB in 0s (4,216 kB/s)\n",
            "Selecting previously unselected package gcc-6-base:amd64.\n",
            "(Reading database ... 164591 files and directories currently installed.)\n",
            "Preparing to unpack .../gcc-6-base_6.5.0-2ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking gcc-6-base:amd64 (6.5.0-2ubuntu1~18.04) ...\n",
            "Selecting previously unselected package libgfortran3:amd64.\n",
            "Preparing to unpack .../libgfortran3_6.5.0-2ubuntu1~18.04_amd64.deb ...\n",
            "Unpacking libgfortran3:amd64 (6.5.0-2ubuntu1~18.04) ...\n",
            "Selecting previously unselected package python3.6-venv.\n",
            "Preparing to unpack .../python3.6-venv_3.6.9-1~18.04ubuntu1.4_amd64.deb ...\n",
            "Unpacking python3.6-venv (3.6.9-1~18.04ubuntu1.4) ...\n",
            "Selecting previously unselected package python3-venv.\n",
            "Preparing to unpack .../python3-venv_3.6.7-1~18.04_amd64.deb ...\n",
            "Unpacking python3-venv (3.6.7-1~18.04) ...\n",
            "Setting up gcc-6-base:amd64 (6.5.0-2ubuntu1~18.04) ...\n",
            "Setting up python3.6-venv (3.6.9-1~18.04ubuntu1.4) ...\n",
            "Setting up python3-venv (3.6.7-1~18.04) ...\n",
            "Setting up libgfortran3:amd64 (6.5.0-2ubuntu1~18.04) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.7/dist-packages (from -r /opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/install_prerequisites/../requirements_caffe.txt (line 1)) (2.5.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from -r /opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/install_prerequisites/../requirements_caffe.txt (line 2)) (1.19.5)\n",
            "Collecting protobuf==3.6.1\n",
            "  Downloading protobuf-3.6.1-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 11.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: defusedxml>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/install_prerequisites/../requirements_caffe.txt (line 4)) (0.7.1)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf==3.6.1->-r /opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/install_prerequisites/../requirements_caffe.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf==3.6.1->-r /opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/install_prerequisites/../requirements_caffe.txt (line 3)) (57.2.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=1.11->-r /opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/install_prerequisites/../requirements_caffe.txt (line 1)) (4.4.2)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.17.3\n",
            "    Uninstalling protobuf-3.17.3:\n",
            "      Successfully uninstalled protobuf-3.17.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.5.0 requires protobuf>=3.9.2, but you have protobuf 3.6.1 which is incompatible.\n",
            "tensorflow-metadata 1.1.0 requires protobuf<4,>=3.13, but you have protobuf 3.6.1 which is incompatible.\n",
            "tensorflow-hub 0.12.0 requires protobuf>=3.8.0, but you have protobuf 3.6.1 which is incompatible.\n",
            "googleapis-common-protos 1.53.0 requires protobuf>=3.12.0, but you have protobuf 3.6.1 which is incompatible.\n",
            "google-api-core 1.26.3 requires protobuf>=3.12.0, but you have protobuf 3.6.1 which is incompatible.\u001b[0m\n",
            "Successfully installed protobuf-3.6.1\n",
            "[WARNING] All Model Optimizer dependencies are installed globally.\n",
            "[WARNING] If you want to keep Model Optimizer in separate sandbox\n",
            "[WARNING] run install_prerequisites.sh venv {caffe|tf|mxnet|kaldi|onnx}\n",
            "\n",
            "\n",
            "###################################################\n",
            "\n",
            "Convert a model with Model Optimizer\n",
            "\n",
            "Run python3 /opt/intel/openvino_2020.1.023/deployment_tools/open_model_zoo/tools/downloader/converter.py --mo /opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/mo.py --name squeezenet1.1 -d /root/openvino_models/models -o /root/openvino_models/ir --precisions FP16\n",
            "\n",
            "========= Converting squeezenet1.1 to IR (FP16)\n",
            "Conversion command: /usr/bin/python3 -- /opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/mo.py --framework=caffe --data_type=FP16 --output_dir=/root/openvino_models/ir/public/squeezenet1.1/FP16 --model_name=squeezenet1.1 '--input_shape=[1,3,227,227]' --input=data '--mean_values=data[104.0,117.0,123.0]' --output=prob --input_model=/root/openvino_models/models/public/squeezenet1.1/squeezenet1.1.caffemodel --input_proto=/root/openvino_models/models/public/squeezenet1.1/squeezenet1.1.prototxt\n",
            "\n",
            "Model Optimizer arguments:\n",
            "Common parameters:\n",
            "\t- Path to the Input Model: \t/root/openvino_models/models/public/squeezenet1.1/squeezenet1.1.caffemodel\n",
            "\t- Path for generated IR: \t/root/openvino_models/ir/public/squeezenet1.1/FP16\n",
            "\t- IR output name: \tsqueezenet1.1\n",
            "\t- Log level: \tERROR\n",
            "\t- Batch: \tNot specified, inherited from the model\n",
            "\t- Input layers: \tdata\n",
            "\t- Output layers: \tprob\n",
            "\t- Input shapes: \t[1,3,227,227]\n",
            "\t- Mean values: \tdata[104.0,117.0,123.0]\n",
            "\t- Scale values: \tNot specified\n",
            "\t- Scale factor: \tNot specified\n",
            "\t- Precision of IR: \tFP16\n",
            "\t- Enable fusing: \tTrue\n",
            "\t- Enable grouped convolutions fusing: \tTrue\n",
            "\t- Move mean values to preprocess section: \tFalse\n",
            "\t- Reverse input channels: \tFalse\n",
            "Caffe specific parameters:\n",
            "\t- Path to Python Caffe* parser generated from caffe.proto: \t/opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/mo/front/caffe/proto\n",
            "\t- Enable resnet optimization: \tTrue\n",
            "\t- Path to the Input prototxt: \t/root/openvino_models/models/public/squeezenet1.1/squeezenet1.1.prototxt\n",
            "\t- Path to CustomLayersMapping.xml: \tDefault\n",
            "\t- Path to a mean file: \tNot specified\n",
            "\t- Offsets for a mean file: \tNot specified\n",
            "Model Optimizer version: \t2020.1.0-61-gd349c3ba4a\n",
            "\n",
            "[ SUCCESS ] Generated IR version 10 model.\n",
            "[ SUCCESS ] XML file: /root/openvino_models/ir/public/squeezenet1.1/FP16/squeezenet1.1.xml\n",
            "[ SUCCESS ] BIN file: /root/openvino_models/ir/public/squeezenet1.1/FP16/squeezenet1.1.bin\n",
            "[ SUCCESS ] Total execution time: 5.81 seconds. \n",
            "[ SUCCESS ] Memory consumed: 114 MB. \n",
            "\n",
            "\n",
            "\n",
            "###################################################\n",
            "\n",
            "Build Inference Engine samples\n",
            "\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Looking for C++ include unistd.h\n",
            "-- Looking for C++ include unistd.h - found\n",
            "-- Looking for C++ include stdint.h\n",
            "-- Looking for C++ include stdint.h - found\n",
            "-- Looking for C++ include sys/types.h\n",
            "-- Looking for C++ include sys/types.h - found\n",
            "-- Looking for C++ include fnmatch.h\n",
            "-- Looking for C++ include fnmatch.h - found\n",
            "-- Looking for strtoll\n",
            "-- Looking for strtoll - found\n",
            "-- Found InferenceEngine: /opt/intel/openvino_2020.1.023/deployment_tools/inference_engine/lib/intel64/libinference_engine.so (Required is at least version \"2.1\") \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /root/inference_engine_samples_build\n",
            "\u001b[35m\u001b[1mScanning dependencies of target format_reader\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target gflags_nothreads_static\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object thirdparty/gflags/CMakeFiles/gflags_nothreads_static.dir/src/gflags_reporting.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object thirdparty/gflags/CMakeFiles/gflags_nothreads_static.dir/src/gflags_completions.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object thirdparty/gflags/CMakeFiles/gflags_nothreads_static.dir/src/gflags.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object common/format_reader/CMakeFiles/format_reader.dir/MnistUbyte.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object common/format_reader/CMakeFiles/format_reader.dir/format_reader.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object common/format_reader/CMakeFiles/format_reader.dir/opencv_wraper.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object common/format_reader/CMakeFiles/format_reader.dir/bmp.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX shared library ../../intel64/Release/lib/libformat_reader.so\u001b[0m\n",
            "[ 72%] Built target format_reader\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX static library ../../intel64/Release/lib/libgflags_nothreads.a\u001b[0m\n",
            "[ 81%] Built target gflags_nothreads_static\n",
            "\u001b[35m\u001b[1mScanning dependencies of target classification_sample_async\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object classification_sample_async/CMakeFiles/classification_sample_async.dir/main.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../intel64/Release/classification_sample_async\u001b[0m\n",
            "[100%] Built target classification_sample_async\n",
            "\n",
            "\n",
            "###################################################\n",
            "\n",
            "Run Inference Engine classification sample\n",
            "\n",
            "Run ./classification_sample_async -d CPU -i /opt/intel/openvino/deployment_tools/demo/car.png -m /root/openvino_models/ir/public/squeezenet1.1/FP16/squeezenet1.1.xml\n",
            "\n",
            "[ INFO ] InferenceEngine: \n",
            "\tAPI version ............ 2.1\n",
            "\tBuild .................. 37988\n",
            "\tDescription ....... API\n",
            "[ INFO ] Parsing input parameters\n",
            "[ INFO ] Parsing input parameters\n",
            "[ INFO ] Files were added: 1\n",
            "[ INFO ]     /opt/intel/openvino/deployment_tools/demo/car.png\n",
            "[ INFO ] Creating Inference Engine\n",
            "\tCPU\n",
            "\tMKLDNNPlugin version ......... 2.1\n",
            "\tBuild ........... 37988\n",
            "\n",
            "[ INFO ] Loading network files\n",
            "[ INFO ] Preparing input blobs\n",
            "[ WARNING ] Image is resized from (787, 259) to (227, 227)\n",
            "[ INFO ] Batch size is 1\n",
            "[ INFO ] Loading model to the device\n",
            "[ INFO ] Create infer request\n",
            "[ INFO ] Start inference (10 asynchronous executions)\n",
            "[ INFO ] Completed 1 async request execution\n",
            "[ INFO ] Completed 2 async request execution\n",
            "[ INFO ] Completed 3 async request execution\n",
            "[ INFO ] Completed 4 async request execution\n",
            "[ INFO ] Completed 5 async request execution\n",
            "[ INFO ] Completed 6 async request execution\n",
            "[ INFO ] Completed 7 async request execution\n",
            "[ INFO ] Completed 8 async request execution\n",
            "[ INFO ] Completed 9 async request execution\n",
            "[ INFO ] Completed 10 async request execution\n",
            "[ INFO ] Processing output blobs\n",
            "\n",
            "Top 10 results:\n",
            "\n",
            "Image /opt/intel/openvino/deployment_tools/demo/car.png\n",
            "\n",
            "classid probability label\n",
            "------- ----------- -----\n",
            "817     0.6853030   sports car, sport car\n",
            "479     0.1835197   car wheel\n",
            "511     0.0917197   convertible\n",
            "436     0.0200694   beach wagon, station wagon, wagon, estate car, beach waggon, station waggon, waggon\n",
            "751     0.0069604   racer, race car, racing car\n",
            "656     0.0044177   minivan\n",
            "717     0.0024739   pickup, pickup truck\n",
            "581     0.0017788   grille, radiator grille\n",
            "468     0.0013083   cab, hack, taxi, taxicab\n",
            "661     0.0007443   Model T\n",
            "\n",
            "[ INFO ] Execution successful\n",
            "\n",
            "[ INFO ] This sample is an API example, for any performance measurements please use the dedicated benchmark_app tool\n",
            "\n",
            "\n",
            "###################################################\n",
            "\n",
            "Demo completed successfully.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TygLw4WIO8BD"
      },
      "source": [
        "### Here we run some modifications in the ssd2 OpenVINO extension for TF so that our Mobilenet SSDv2 model can convert successfully to the IR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-bGZHGcMNbZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35db1c2d-c570-432d-cb37-6e93dd978595"
      },
      "source": [
        "%cd /opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/\n",
        "#openvino fixes: edit \n",
        "# Read in the file, make sure the .json corresponds to the model!!!\n",
        "with open('ssd_v2_support.json', 'r') as file :\n",
        "  filedata = file.read()\n",
        "\n",
        "# Replace the target string\n",
        "filedata = filedata.replace('\"Postprocessor/ToFloat\"', '\"Postprocessor/Cast_1\"')\n",
        "\n",
        "# Write the file out again\n",
        "with open('ssd_v2_support.json', 'w') as file:\n",
        "  file.write(filedata)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/extensions/front/tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROig0wlCkxoX"
      },
      "source": [
        "#%cp \"/content/models/research/fine_tuned_model/saved_model/saved_model.pb\" \"/content/models/research/fine_tuned_model/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeuKgVzVZWPY"
      },
      "source": [
        "%cp -r \"/content/drive/MyDrive/Data/models/TF4/fine_tuned_model/\" \"/content/models/research/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7sqTX79W6hh"
      },
      "source": [
        "## Convert TF model to Open Vino Intermediate Representation\n",
        "If using own model, please change to your desired name for output directory --output_dir \"choose name\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsWggE5AIWS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6109e0f-e93e-443d-b664-d99ddf68fd93"
      },
      "source": [
        "#CONVERT TF MODEL to OPEN VINO IRv10. saved in IR_V10_fruits_mnssdv2_6k directory or\n",
        "#choose own name for --output_dir \"choose name\"\n",
        "%cd \"/content/models/research/fine_tuned_model/\"\n",
        "!source /opt/intel/openvino/bin/setupvars.sh && \\\n",
        "    python /opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
        "    --input_model frozen_inference_graph.pb \\\n",
        "    --tensorflow_use_custom_operations_config /opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json \\\n",
        "    --tensorflow_object_detection_api_pipeline_config pipeline.config \\\n",
        "    --reverse_input_channels \\\n",
        "    --output_dir '/content/out' \\\n",
        "    --data_type FP16"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/models/research/fine_tuned_model\n",
            "[setupvars.sh] OpenVINO environment initialized\n",
            "[ WARNING ]  Use of deprecated cli option --tensorflow_use_custom_operations_config detected. Option use in the following releases will be fatal. Please use --transformations_config cli option instead\n",
            "Model Optimizer arguments:\n",
            "Common parameters:\n",
            "\t- Path to the Input Model: \t/content/models/research/fine_tuned_model/frozen_inference_graph.pb\n",
            "\t- Path for generated IR: \t/content/out\n",
            "\t- IR output name: \tfrozen_inference_graph\n",
            "\t- Log level: \tERROR\n",
            "\t- Batch: \tNot specified, inherited from the model\n",
            "\t- Input layers: \tNot specified, inherited from the model\n",
            "\t- Output layers: \tNot specified, inherited from the model\n",
            "\t- Input shapes: \tNot specified, inherited from the model\n",
            "\t- Mean values: \tNot specified\n",
            "\t- Scale values: \tNot specified\n",
            "\t- Scale factor: \tNot specified\n",
            "\t- Precision of IR: \tFP16\n",
            "\t- Enable fusing: \tTrue\n",
            "\t- Enable grouped convolutions fusing: \tTrue\n",
            "\t- Move mean values to preprocess section: \tFalse\n",
            "\t- Reverse input channels: \tTrue\n",
            "TensorFlow specific parameters:\n",
            "\t- Input model in text protobuf format: \tFalse\n",
            "\t- Path to model dump for TensorBoard: \tNone\n",
            "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
            "\t- Update the configuration file with input/output node names: \tNone\n",
            "\t- Use configuration file used to generate the model with Object Detection API: \t/content/models/research/fine_tuned_model/pipeline.config\n",
            "\t- Operations to offload: \tNone\n",
            "\t- Patterns to offload: \tNone\n",
            "\t- Use the config file: \t/opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json\n",
            "Model Optimizer version: \t2020.1.0-61-gd349c3ba4a\n",
            "2021-07-29 08:58:31.421178: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "[ WARNING ]  \n",
            "Detected not satisfied dependencies:\n",
            "\ttensorflow: installed: 2.5.0, required: < 2.0.0\n",
            "\n",
            "Please install required versions of components or use install_prerequisites script\n",
            "/opt/intel/openvino_2020.1.023/deployment_tools/model_optimizer/install_prerequisites/install_prerequisites_tf.sh\n",
            "Note that install_prerequisites scripts may install additional components.\n",
            "The Preprocessor block has been removed. Only nodes performing mean value subtraction and scaling (if applicable) are kept.\n",
            "\n",
            "[ SUCCESS ] Generated IR version 10 model.\n",
            "[ SUCCESS ] XML file: /content/out/frozen_inference_graph.xml\n",
            "[ SUCCESS ] BIN file: /content/out/frozen_inference_graph.bin\n",
            "[ SUCCESS ] Total execution time: 37.30 seconds. \n",
            "[ SUCCESS ] Memory consumed: 494 MB. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9R7LTkNvZos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a2cf7d-e04a-4478-c279-84ffe9467e57"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P15Hl-Fbmkib",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc368e2d-bcbb-48a4-adc3-cca57fcedda9"
      },
      "source": [
        "#check directory containing the exported TF trained model and the IRv10 folder\n",
        "%ls "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint                      model.ckpt.index  saved_model.pb\n",
            "\u001b[0m\u001b[01;34mfine_tuned_model\u001b[0m/               model.ckpt.meta   \u001b[01;34msigns\u001b[0m/\n",
            "frozen_inference_graph.pb       pipeline.config\n",
            "model.ckpt.data-00000-of-00001  \u001b[01;34msaved_model\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD2BMtXoPjYN"
      },
      "source": [
        "## Now we compile the IR model to a .blob for use on DepthAI modules/platform\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqZqajTIP3Sv"
      },
      "source": [
        "### We save the blob in the IR directory from above, corresponding to --output_dir parameter above. \n",
        "The blob filename will be *frozen_inference_graph.blob*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBDt-QKvlcdO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ccdd2a-c6be-4fe2-b28e-81886da5c3bc"
      },
      "source": [
        "%ls /content/models/research/fine_tuned_model/signs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "frozen_inference_graph.bin      frozen_inference_graph.xml\n",
            "frozen_inference_graph.mapping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqiPpnCwPioH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "8eb50e85-5418-495e-e6a9-16e99376885c"
      },
      "source": [
        "#No changes needed here unless using custom data.\n",
        "#CHOOSE the directory where you would like to save the blob.\n",
        "# I use the same --output_dir as above for the IR conversion\n",
        "blob_dir = \"/content/out/\"\n",
        "\n",
        "#Copy the path of your .xml and .bin files. For that, you can look at the IR\n",
        "#conversion output cell, select and copy from:\n",
        "#[SUCCESS] XML file and bin file paths.\n",
        "#Or you can choose to compile other .xml .bin files from a different location\n",
        "#\n",
        "xmlfile = \"/content/out/frozen_inference_graph.xml\"\n",
        "binfile = \"/content/out/frozen_inference_graph.bin\"\n",
        "\n",
        "import requests\n",
        "\n",
        "#For openvino 20.01 use this link to compile the blob\n",
        "url = \"http://69.164.214.171:8080\"\n",
        "\n",
        "\n",
        "#open vino 20.02 link:\n",
        "# url = \"69.164.214.171:8081\"\n",
        "\n",
        "payload = {'compiler_params': '-ip U8 -VPU_MYRIAD_PLATFORM VPU_MYRIAD_2480 -VPU_NUMBER_OF_SHAVES 4 -VPU_NUMBER_OF_CMX_SLICES 4',\n",
        "'compile_type': 'myriad'\n",
        "}\n",
        "files = [\n",
        "  ('definition', open(xmlfile,'rb')),\n",
        "  ('weights', open(binfile,'rb'))\n",
        "]\n",
        "# headers = {\n",
        "#   'Content-Type': 'application/json'\n",
        "# }\n",
        "response = requests.post( url, data = payload, files = files)\n",
        "blobnameraw = response.headers.get('Content-Disposition')\n",
        "print(blobnameraw)\n",
        "blobname = blobnameraw[blobnameraw.find('='):][1:]\n",
        "with open(blob_dir + blobname, 'wb') as f:\n",
        "  f.write(response.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1326\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1074\u001b[0m                         \u001b[0;34m+\u001b[0m \u001b[0;34mb'\\r\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_method_retryable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1326\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1074\u001b[0m                         \u001b[0;34m+\u001b[0m \u001b[0;34mb'\\r\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1076\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    996\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', BrokenPipeError(32, 'Broken pipe'))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-29537cdec509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m#   'Content-Type': 'application/json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpayload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mblobnameraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Content-Disposition'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblobnameraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \"\"\"\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', BrokenPipeError(32, 'Broken pipe'))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "en_RBRNfcLDj"
      },
      "source": [
        "import requests\n",
        "\n",
        "url = \"http://69.164.214.171:8080/compile\"  # change if running against other URL\n",
        "\n",
        "payload = {\n",
        "    'compiler_params': '-ip U8 -VPU_MYRIAD_PLATFORM VPU_MYRIAD_2480 -VPU_NUMBER_OF_SHAVES 4 -VPU_NUMBER_OF_CMX_SLICES 4',\n",
        "    'compile_type': 'myriad'\n",
        "}\n",
        "files = {\n",
        "    'definition': open('/content/out/frozen_inference_graph.xml', 'rb'),\n",
        "    'weights': open('/content/out/frozen_inference_graph.bin', 'rb')\n",
        "}\n",
        "params = {\n",
        "    'version': '2020.1',  # OpenVINO version, can be \"2021.1\", \"2020.4\", \"2020.3\", \"2020.2\", \"2020.1\", \"2019.R3\"\n",
        "}\n",
        "\n",
        "response = requests.post(url, data=payload, files=files, params=params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVB2Nzl_fgrA",
        "outputId": "298c5372-e39a-461a-babb-a4072d327574"
      },
      "source": [
        "response.content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\\n<title>500 Internal Server Error</title>\\n<h1>Internal Server Error</h1>\\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24nm3eKVfkPh"
      },
      "source": [
        "res "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qJ0knjImLIn"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/models/research/fine_tuned_model/signs/frozen_inference_graph.blob')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofNMzGiO3LrA"
      },
      "source": [
        "cp -r /content/models/research/fine_tuned_model/ /content/drive/MyDrive/Data/models/TF4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mAuFdfT415I",
        "outputId": "8b575aa0-7150-4300-9e5a-8f50f794fb4e"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint                      model.ckpt.index  \u001b[0m\u001b[01;34msaved_model\u001b[0m/\n",
            "frozen_inference_graph.pb       model.ckpt.meta   saved_model.pb\n",
            "model.ckpt.data-00000-of-00001  pipeline.config   \u001b[01;34msigns\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3JUawGSs458z",
        "outputId": "fd0b8cbe-9ed8-4d5e-cf06-f33cb414aef8"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/models/research/fine_tuned_model'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24-2kaAFvyEd"
      },
      "source": [
        "## To run the .blob in DepthAI, we proceed step 5 here:\n",
        " https://docs.luxonis.com/tutorials/object_det_mnssv2_training/"
      ]
    }
  ]
}